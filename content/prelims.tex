\chapter{Preliminaries}
In this chapter we define and discuss the basic mathematical concepts needed for this dissertation.

\section{Notation}
TODO

\section{Semidefinite Programming and Duality}
At the heart of this thesis is investigating the power of semidefinite programming in general, and the specific Sum-of-Squares SDP in particular.
In this section we define semidefinite programs and their duals, which are also semidefinite programs. 

\begin{definition}
A \emph{semidefinite program (SDP) of size $d$} is a tuple $(C,\{A_i, b_i\}_{i=1}^m)$ where $C,A_i \in R^{d \times d}$ for each $i$, and $b_i \in \R$ for each $i$.
The \emph{feasible region} of the SDP is the set $S = \{X | \forall i: A_i \cdot X = b_i, X \in \mathbb{S}_+^d\}$.
The \emph{value} of the SDP is $\max_{X\in S} C \cdot X$. 
\end{definition}
\begin{fact}
There is an algorithm (referred to as the Ellipsoid Algorithm in this thesis) that, given an SDP $(C, \{A_i,b_i\}_{i=1}^m)$ whose feasible region $S$ is contained in a ball of radius $R$, computes the value of that SDP up to accuracy $\epsilon$ in time polynomial in $\max_i\left(\|A_i\|,b_i\right)$, $\|C\|$, $R$, and $\frac{1}{\epsilon}$.
\end{fact}

\begin{definition}
The \emph{dual} of an SDP $(C, \{A_i, b_i\}_{i=1}^m)$ is the optimization problem (with variables $(y,S)$): 
\begin{align*}
&\min b \cdot y \\
&\sum_i A_i y_i - C = S \\
&S \succeq 0.
\end{align*}
The \emph{value} of the dual is the value of the optimum $b \cdot y^*$. 
\end{definition}
The following is a well-known fact about duality for SDPs. 
\begin{lemma}\prettyref{lem:duality}
Let $D$ be the dual of an SDP $P = (C, \{A_i, b_i\}_{i=1}^m)$. If $X$ is feasible for $P$ and $(y,S)$ is feasible for $D$, then $C \cdot X \leq b \cdot y$. Moreover, if there exists a \emph{strictly feasible} point $X$ for $P$ or $(y,S)$ $D$, that is a feasible $X$ with $X \succ 0$ or a feasibly $(y,S)$ with $S \succ 0$, then $\val{P} = \val{D}$.
\end{lemma}

\section{Polynomial Ideals and Polynomial Proof Systems}\label{sec:polyproofs}
We write $p(x)$ or sometimes just $p$ for a polynomial in $\R[x_1,\dots,x_n]$, and $\cP$ for a set of polynomials.
We will often also use $q$ and $r$ for polynomials and $\cQ$ for a second set of polynomials. 
\begin{definition}
Let $\cP,\cQ$ be any set of polynomials in $\R[x_1,\dots,x_n]$, and let $S$ be any set of points in $\R^n$.
\begin{itemize}
\item We call $V(\cP) = \{x \in \R^n | \forall p \in \cP: p(x) = 0\}$ the \emph{real variety} of $\cP$. 
\item We call $H(\cQ) = \{x \in \R^n | \forall q \in \cQ: q(x) \geq 0\}$ the \emph{positive set} of $\cQ$.
\item We call $I(S) = \{p \in R[x_1,\dots,x_n] | \forall x \in S: p(x) = 0\}$ the \emph{vanishing ideal} of $S$.
\item We denote $\gen{\cP} = \{q \in R[x_1,\dots,x_n] | \exists \lambda_p(x): q = \sum_{p \in \cP} \lambda_p \cdot p\}$ for the \emph{ideal generated by $\cP$}.
\item We call $\cP$ \emph{complete} if $\gen{\cP} = I(V(\cP))$.
\item If $\cP$ is complete, then we say $p_1 \cong p_2 \mod \gen{\cP}$ if $p_1 - p_2 \in \gen{\cP}$ or equivalently, if $p_1(\alpha) = p_2(\alpha)$ for each $\alpha \in V(\cP)$.
\end{itemize}
\end{definition}

\begin{definition}\label{def:grobner}
Let $\succ$ be an ordering on monomials such that if $x_U \succ x_V$ then $x_Ux_W \succ x_Vx_W$. We say that $\cP$ is a \emph{Gr\"obner Basis} for $\gen{\cP}$ (with respect to $\succ$) if, for every $r \in \gen{\cP}$, there exists a $p \in \cP$ such that the leading term of $r$ is divisble by the leading term of $p$.
\end{definition}
\begin{example}\label{ex:grobner}
Consider the polynomials on $n$ variables $x_1,\dots,x_n$ and let $\succ$ be the degree-lexicographic ordering, so that $x_U \succ x_V$ if the vector of degrees of $x_U$ is larger than the vector of $x_V$ in the lexicographic ordering. Then $\cP = \csp$ is a Gr\"obner Basis. The proof is in the proof of \prettyref{cor:csp-effective}.
\end{example}
If $\cP$ is a Gr\"obner basis, then it completely explains $\gen{\cP}$ in the sense that it is possible to define a multivariate division algorithm for $\gen{\cP}$ with respect to $\cP$. 
\begin{definition}
Let $\succ$ be an ordering of monomials such that if $x_U \succ x_V$ then $x_Ux_W \succ x_Vx_W$. We say a polynomial $q$ is \emph{reducible} by a set of polynomials $\cP$ if there exists a $p \in \cP$ such that any term of $q$, $c_Qx_Q$, is divisble by the \emph{leading} term of $p$, $c_Px_P$. Then a \emph{reduction} of $q$ by $\cP$ is $q - \frac{c_Q}{c_P}x_{Q \setminus P}p$. We say that a \emph{total reduction} of $q$ by $\cP$ is a polynomial obtained by iteratively applying reductions until we reach a polynomial which is not reducible by $\cP$. 
\end{definition}
In general the complete reductions of a polynomial $q$ by a set of polynomials $\cP$ is not unique and depends on which polynomials one chooses from $\cP$ to reduce by, and in what order. So it does not make much sense to call this a division algorithm since there is not a unique remainder. However, when $\cP$ is a Gr\"obner basis, we can call it division.
\begin{proposition}\label{prop:grobner-unique}
Let $\cP$ be a Gr\"obner basis for $\gen{\cP}$ with respect to $\succ$. Then for any polynomial $q$, there is a unique total reduction of $q$ by $\cP$. In particular if $q \in \gen{\cP}$, then the total reduction of $q$ by $\cP$ is $0$. The converse is also true, so if $\cP$ is a set of polynomials such that any polynomial $q \in \gen{\cP}$ has unique total reduction by $\cP$ equal to $0$, then $\cP$ is a Gr\"obner basis. 
\end{proposition}
\begin{proof}
When we reduce a polynomial $q$ by $\cP$, the resulting polynomial does not contain one term of $q$, and since it was canceled with a multiple of the leading term of a polynomial $p$, no higher monomials were introduced. Thus as we apply reduction, the position of the terms of $q$ monotonically decrease. This has to terminate at some point, so there is a remainder $r$ which is not reducible by $\cP$. To prove that $r$ is unique, first notice that the result of total reduction is a polynomial identity $q = p + r$, where $p \in \gen{\cP}$ and $r$ is not reducible by $\cP$. If there are multiple remainders $q = p_1 + r_1$ and $q = p_2 + r_2$, then clearly $r_1 - r_2 = p_2 - p_1 \in \gen{\cP}$. By the definition of Gr\"obner Basis, $r_1 - r_2$ must have its leading term divisble by the leading term of some $p \in \cP$. But the leading term of $r_1 - r_2$ must come from either $r_1$ or $r_2$, neither of which contain terms divisble by leading terms of any polynomial in $\cP$. Thus $r_1 - r_2 = 0$.

For the converse, let $q \in \gen{\cP}$, and note again that any reduction of $q$ by a polynomial in $\cP$ does not include higher monomials than the one canceled. Since the only total reduction of $q$ is $0$, its leading term has to be canceled eventually, so it must be divisible by the leading term of some polynomial in $\cP$.
\end{proof}

\subsection{Testing Zero Polynomials}

This section discusses how to certify that a polynomial $r(x)$ is zero on all of some set $S$. Later in \prettyref{cha:symmetric_sdps} we will see that this can be used to show that a specific SDP relaxation achieves the best approximation among small, symmetric SDP relaxations. 
In these cases we often have access to some polynomials $\cP$ such that $S = V(\cP)$.
When $\cP$ is complete, testing if $r$ is zero on $S$ is equivalent to testing if $r \in \gen{\cP}$.
One obvious way to do this is to simply brute force over the points of $V(\cP)$ and evaluate $r$ on all of them. 
However, we are mostly interested in situations where the points of $V(\cP)$ are in bijection with solutions to some combinatorial optimization problem.
In this case, there are frequently an exponential number of points in $V(\cP)$ and this amounts to a brute force search over this space. 
We want a more efficient certificate for membership in $\gen{\cP}$. 

\begin{definition}
Let $\cP = \{p_1, p_2, \dots, p_n\}$ be a set of polynomials. We say that \emph{$r$ is derived from $\cP$ in degree $d$} if there is a polynomial identity of the form
\[r(x) = \sum_{i=1}^n \lambda_i(x) \cdot p_i(x),\]
and $\max_i \deg(\lambda_i \cdot p_i) \leq d$. We often call this polynomial identity a Polynomial Calculus (PC) proof, derivation, or certificate from $\cP$.
\end{definition} 

The problem of finding a degree-$d$ PC derivation for $r$ can be expressed as a linear program with $n^d$ variables, since the polynomial identity is linear in the coefficients of the $\lambda_i$. Thus if such a derivation exists, it is possible to find efficiently in time polynomial in $n^d$, $\|\cP\|$ and $\|r\|$. 
\begin{definition}
We say that $\cP$ is \emph{$k$-effective} if $\cP$ is complete and every polynomial $p \in \gen{\cP}$ of degree $d$ has a PC proof from $\cP$ in degree $kd$.
\end{definition}
When $\cP$ is $k$-effective, if we ever wish to test membership in $\gen{\cP}$ for some polynomial $r$, we need only search for a PC proof up to degree $k \deg r$, yielding an efficient algorithm for the membership problem. This is the main motivation behind developing techniques to prove that a set of polynomials $\cP$ is $k$-effective. In \prettyref{cha:effective_derivations} we prove that many sets of polynomials correpsonding to optimization problems are effective. We will see two important consequences of effective sets of polynomials in \prettyref{cha:bit_complexity} and \prettyref{cha:symmetric_sdps}.

\subsection{Testing Nonnegative Polynomials with Sum of Squares}

Testing nonnegativity for polynomials on a set $S$ has a much more obvious application to optimization. If one is trying to solve the polynomial optimization problem
\[\begin{tabular}{ll} $\max r(x)$ & subject to \\ $\forall p \in \cP: p(x) = 0$ & $\forall q \in \cQ: q(x) \geq 0$ \end{tabular}\]
then one way to do so is to iteratively pick a $\theta$ and test whether $\theta - r(x)$ is positive on $S = V(\cP) \cap H(\cQ)$. 
If we pick $\theta$ in order to perform binary search, we can find $\max r(x)$ very quickly. One way to try and certify nonnegative polynomials is to express them as sums of squares.

\begin{definition}
A polynomial $s(x) \in \R[x_1,\dots,x_n]$ is called a \emph{sum-of-squares (or SOS)} polynomial if $s(x) = \sum_i h_i^2(x)$ for some polynomials $h_i \in \R[x_1,\dots,x_n]$. We often use $s(x)$ to denote SOS polynomials. 
\end{definition}

Clearly an SOS polynomial is nonnegative on all of $\R^n$. However, the converse is not always true.
\begin{fact}[\cite{Motzkin}]
The polynomial $p(x,y) = x^4y^2 + x^2y^4 - 3x^2y^2 + 1$ is nonnegative on $\R^n$ but is not a sum of squares.
\end{fact}

In our optimization motivation, we actually only care about the nonnegativity of a polynomial on a set $S$ rather than on all of $\R^n$. 

\begin{definition}
A polynomial $r(x) \in \R[x_1,\dots,x_n]$ is called \emph{SOS modulo $S$} if there is an SOS polynomial $s \in I(S)$ such that $r \cong s \mod I(S)$. If $\deg s = k$ then we say $r$ is \emph{$k$-SOS modulo $S$}. If $S = V(\cP)$ and $\cP$ is complete, we sometimes use \emph{modulo $\cP$} instead.
\end{definition}
If a polynomial $r$ is SOS modulo $S$ then $r$ is nonnegative on $S$. In the situations discussed in this thesis, often $S \subseteq \{0,1\}^n$. In this case, the converse holds.
\begin{fact}\label{fact:boolSOS}
If $S \subseteq \{0,1\}^n$ and $r$ is a polynomial which is nonnegative on $S$, then $r$ is $n$-SOS modulo $S$. 
\end{fact}

When we have access to two sets of polynomials $\cP$ and $\cQ$ such that $S = V(\cP) \cap H(\cQ)$, as in our main motivation of polynomial optimization, we can define a certificate of nonnegativity:

\begin{definition}
Let $\cP$ and $\cQ$ be two sets of polynomials. A polynomial $r(x)$ is said to have a \emph{degree $d$ proof of nonnegativity from $\cP$ and $\cQ$} if there is a polynomial identity of the form
\[r(x) = s(x) + \sum_{q \in \cQ} s_q(x) \cdot q(x) + \sum_{p \in \cP} \lambda_p(x) \cdot p(x),\]
where $s(x)$, and each $s_q(x)$ are SOS polynomials, and $\max{\deg s, \deg s_qq, \deg \lambda_pp} \leq d$. We often call this polynomial identity a Positivestellensatz Calculus (PC$_>$) proof of nonnegativity, derivation, or certificate from $\cP$ and $\cQ$.
We often identify the proof with the set of polynomials $\Pi = \{s\} \cup \{s_q | q\in \cQ\} \cup \{\lambda_p | p \in \cP\}$.

If $\cQ = \emptyset$ and both $r(x)$ and $-r(x)$ have PC$_>$ proofs of nonnegativity from $\cP$, then we say that $r$ has a PC$_>$ derivation.
\end{definition}

If $r$ has a PC$_>$ proof of nonnegativity from $\cP$ and $\cQ$, then $r$ is nonnegative on $S = V(\cP) \cap H(\cQ)$. This can be seen by noticing that the first two terms in the proof are nonnegative because they are sums of products of polynomials which are nonnegative on $S$, and the final term is of course zero on $S$ because it is in $\gen{\cP}$. 

The problem of finding a degree-$d$ proof of nonnegativity can be expressed as a semidefinite program of size $d|\cQ|$ since a polynomial is SOS if and only if its matrix of coefficients is PSD. Then the Ellipsoid Alogorithm can be used to find a degree-$d$ proof of nonnegativity $\Pi$ in time polynomial in $\|r\|, \|\cP\|, \|\cQ\|, n^{d|\cQ|}$, and $\|\Pi\|$. The quantity $\|\Pi\|$ is worrisome; it is not part of the input unlike $\cP$ and $\cQ$, and \emph{a priori} we have no way to bound its size. One way to certify a proof of bounded norm is of course to simply exhibit one. If we suspect there are no proofs with small norm, there are also certificates we can find:
\begin{lemma}\label{lem:prelim_dual_cert}
Let $\cP$ and $\cQ$ be sets of polynomials and $r(x)$ be a polynomial. Pick any $p^* \in \cP$. If there exists a linear functional $\phi: \R[x_1,\dots,x_n] \rightarrow \R$ such that 
\begin{enumerate}
\item[(1)] $\phi[r] = -\epsilon < 0$,
\item[(2)] $\phi[\lambda p] = 0$ for every $p \in \cP$ except $p^*$ and $\lambda$ such that $\deg \lambda p \leq 2d$,
\item[(3)] $\phi[s^2q] \geq 0$ for every $q \in \cQ$ and polynomial $s$ such that $\deg s^2q \leq 2d$,
\item[(4)] $\phi[s^2] \geq 0$ for every polynomial $s$ such that $\deg s^2 \leq 2d$,
\item[(5)] $|\phi[\lambda p^*]| \leq \delta \|\lambda\|$ for every $\lambda$ such that $\deg \lambda p^* \leq 2d$,
\end{enumerate}
then every degree $d$ PC$_>$ proof of nonnegativity $\Pi$ for $r$ from $\cP$ and $\cQ$ has $\|\Pi\| \geq \frac{\epsilon}{\delta}$.
\end{lemma}
\begin{proof}
The proof is very simple. Any degree $d$ proof of nonnegativity for $r$ is a polynomial identity
\[r(x) = s(x) + \sum_{q \in \cQ} s_q(x)\cdot q(x) + \sum_{\substack{p \in \cP \\ p \neq p^*}} \lambda_p(x) \cdot p(x) + \lambda^*(x) \cdot p^*(x)\]
with polynomial degrees appropriately bounded. If we apply $\phi$ to both sides, we have 
\begin{align*}
-\epsilon = \phi[r] &= \phi[s] + \sum_{q \in \cQ} \phi[s_qq] + \sum_{\substack{p \in \cP \\ p \neq p^*}} \phi[\lambda_p p] + \phi[\lambda^*p] \\
&= a_1 + a_2 + 0 + \phi[\lambda^* p^*],
\end{align*}
where $a_1,a_2 \geq 0$ by properties (3) and (4) of $\phi$. Thus $\phi[\lambda^* p^*] \leq -\epsilon$, but by property (5), we must have $\|\lambda^*\| \geq \frac{\epsilon}{\delta}$, and thus $\|\Pi\|$ is at least this much as well.
\end{proof}
Strong duality actually implies that the converse of \prettyref{lem:prelim_dual_cert} is true as well (clever readers will notice that $\phi$ is a feasible solution to the dual of the SDP that finds degree $d$ PC$_>$ proofs), but as we only need this direction in this thesis we omit the proof of the converse. In
\prettyref{cha:bit_complexity} we further explore the problem of finding PC$_>$ proofs of bounded norm. We present some of the first theory for finding proofs of bounded norm, and we paint a promising picture for many relevant sets $\cP$ and $\cQ$.

\section{Combinatorial Optimization Problems}\label{sec:prelims_comb_opt}
We define maximization problems here, but it is clear that the definition extends easily to minimization problems as well.
\begin{definition}
An \emph{combinatorial maximization problem} \(\cM = (\cS, \cF)\)
consists of a finite set
\(\cS\) of feasible solutions and a set \(\cF\) of nonnegative
objective functions. An algorithm for such a problem takes as input an $f \in \cF$ and computes
$\max_{s \in \cS} f(s)$.

We can also generalize to approximate solutions: Given two functions \(\tilde{C}, \tilde{S} \colon \cF \rightarrow \R\)
called approximation guarantees, we say
an algorithm \((\tilde{C}, \tilde{S})\)-approximately solves \(\cM\) or achieves approximation $(\tilde{C}, \tilde{S})$ on $\cM$
if given any \(f \in \cF\) with \(\max_{s \in \cS} f(s) \leq \tilde{S}(f)\) as input,
it computes \(\tilde{f}\in \R\) satisfying
\(\max_{s \in \cS} f(s) \leq \tilde{f} \leq \tilde{C}(f)\).
\end{definition}

We think of the functions $f \in \cF$ as defining the problem instances and the feasible solutions $s \in \cS$ as defining the combinatorial objects we are 
trying to maximize over. $\tilde{C}$ and $\tilde{S}$ can be thought of as the usual approximation parameters \emph{completeness} and \emph{soundness}. Here are few concrete examples of combinatorial maximization problems:
\begin{example}
Recall that the Maximum Matching problem is, given a graph $G = (V,E)$, find a maximum set of disjoint edges. We can express this as a combinatorial optimization problem for each $n$ as follows: $K_n$ be the complete graph on $n$ vertices. The set of feasible solutions $\cS$ is the set of all maximum matchings on $K_n$. The objective functions will be indexed by edge subsets of $K_n$ and defined $f_E(M) = |E \cap M|$. It is clear that for a graph $G = (V, E)$ with $|V| = n$, the size of the maximum matching in $G$ is exactly $\max_{M \in \cS} f_E(M)$.
\end{example}
\begin{example}
The Traveling Salesperson Problem (TSP) is, given a set $X$ and a function $c: X\times X \rightarrow \R^+$, find a permutation $\pi$ of $X$ that minimizes the total cost of adjacent pairs in the permutation (including the first and last elements). This can be cast in this framework easily: the set of feasible solutions $\cS$ is the set of all permutations of $n$ elements. The objective functions are indexed by the function $c$ and can be written $f_c(\pi) = \sum_{i=1}^n c(\pi(i),\pi(i+1))$, where $n+1$ is taken to be $1$. TSP is a minimization problem rather than a maximization problem, so we ask for the algorithm to compute $\min_{\pi \in \cS} f(\pi)$ instead. 
\end{example}
\begin{definition}
For a problem $\cM = (\cS, \cF)$ and approximation guarantees $C,S$, the \emph{$(C,S)$-Slack Matrix} $M$ is an operator that takes as input an $s \in \cS$ and an $f \in \cF$ such that $\max_{s \in \cS} f(s) \leq S(f)$ and returns $M(s,f) = C(f) - f(s)$. 
\end{definition}
The slack matrix encodes the combinatorial properties of $\cM$, and we will see in the next section that certain propreties of the slack matrix correspond to the existence of specific algorithms to $(C,S)$-approximately solve $\cM$. In particular, in this thesis we investigate the power of SDPs as a way to solve $\cM$. 

\section{SDP Relaxations for Optimization Problems}\label{prelims_sdp_relaxations}
A popular method for solving combinatorial optimization problems is to formulate them as SDPs, and use the generic algorithms for solving SDPs, see for example \cite{}
One of the main contributions of this dissertation is to prove some of the limits of this method, for example in \prettyref{cha:symmetric_sdps} we will
that the Maximum Matching problem cannot be efficiently solved this way. To give a precise statement, we need to define SDP relaxations.
\begin{definition}
Let $\cM = (\cS, \cF)$ be a combinatorial maximization problem. Then an
  SDP relaxation of \(\cM\)
  of size \(d\)
  consists of constraints $\{A_i, b_i\}_{i=1}^m$ and a set of affine objective functions $w^f(X)$ such that
	\begin{enumerate}
	\item Each $(w^f, \{A_i, b_i\}_{i=1}^m)$ is an SDP of size $d$,
	\item For each $s \in \cS$, there is an associated $X^s$ in the feasible region of the SDP satisfying $w^f(X^s) = f(s)$ for each $f$.
	\end{enumerate}
	
	We say that the SDP relaxation is a $(C,S)$-approximate relaxation or that it achieves $(C,S)$-approximation if, for each
	$f \in \cF$ with $\max_{s \in \cS} f(s) \leq S(f)$, 
	\[\max \face{ w^f(X) \mid \forall i: A_i \cdot X = b_i, X \in \mathbb{S}_+^d} \leq C(f).\]
	If the SDP relaxation achieves a $(\max_{s\in \cS} f, \max_{s\in \cS} f)$-approximation, we say it is \emph{exact}.
\end{definition}
Given a $(\tilde{C}, \tilde{S})$-approximate SDP formulation for $\cP$, we can $(\tilde{C}, \tilde{S})$-approximately solve $\cP$ on input $f$ simply by solving
the SDP $\max w^f(X)$ subject to $X \in \psd{d}$ and $\forall i: A_i(X) = b_i$.

\begin{example}
We can embed any polytope in $n$ dimensions with $d$ facets into the PSD cone of size $2n+d$ and get an exact SDP relaxation for the optimization problem that maximizes linear functions over the vertices of $P$. Let
\[P = \conv\{s\} = \{x: Ax \leq b\},\]
where $A$ is a $d \times n$ matrix. Then we can define new variables $x_i^+$ and $x_i^-$ for each $i \in [n]$ and $z_j$ for each $j \in [d]$. Then for any vector $l \in \R^n$, let $l' = \diag{a_i,-a_i,0,0,\dots,1,0,0,\dots,0}$, where the last block has a $1$ where the $i$th zero would be. In other words, 
\[l' \cdot (x_1^+,x_2^+,\dots,x_n^+,x_1^-,x_2^-,\dots,x_n^-,s_1,s_2,\dots,s_d) = l \cdot (x^+ - x^-) + z_i.\]
Now for any For each vertex $s$ of $P$, there is a $(x_s^+,x_s^-,z_s)$ such that $(x_s^+ - x_s^-) = s$. Thus the SDP $\diag{a_i'} \cdot X = b_i$, $X = \diag{x^+,x^-,z}$, $X \succeq 0$ with feasible solutions $X^s = \diag{x_s^+,x_s^-,z_s}$ and objective functions $w^l(X) = \diag{l'} \cdot X$ is an SDP relaxation for maximizing any linear function over the vertices of $P$. It is easy to see that it is exact.

%The Matching Polytope is an exact formulation of the Maximum Matching problem. The Matching Polytope is defined as follows for each $n \in \Z$: Let $M$ be a maximum matching of the complete graph $K_n$, and let $\chi_M \in \R^{\binom{n}{2}}$ be the characteristic vector of this matching, i.e. $(\chi_M)_{ij} = 1$ iff $(i,j) \in M$. The matching polytope is $\conv\face{\chi_M | M \text{ a matching of $K_n$}}$. To embed this in the cone of PSD matrices rather than the positive orthant, we simply take $P_n = \conv\face{\diag{\chi_M} | M \text{ a matching of $K_n$}}$. Because $P_n$ is a polytope, there is another description $P_n = \{X: \forall i: A_i(X) \leq b_i\}$. To get the same form as in our definition, TODO
%This can be seen to be an SDP formulation by taking the feasible solutions to be $\chi_M$, the objective function $f_E$ can be expressed as $w^{f_E} = \sum_{ij \in E} x_{ij}$. The achievement guarantee is satisfied because $\max_{x \in P_n} w^{f_E}(x)$ must be achieved at a vertex, and any vertex of the polytope must be a $\chi_M$, so $\max_{x \in P_n} w^{f_E}(x) = w^{f_E}(\chi_{M^*}) = f_E(M^*) = \max_{M \in \cS} f_E(M)$. 
\end{example}
\begin{theorem}[Yannakakis' Factorization Theorem]\label{thm:yannakakis}
Let $\cM$ be a combinatorial optimization problem with $(C,S)$-Slack Matrix $M(s,f)$. There exists an $(C,S)$-approximate SDP relaxation of size $d$ for $\cM$ if and only if there exists $X^s,Y_f \in \psd{d}$ and $\mu_f \in \R_+$ such that $X^s \cdot Y_f + \mu_f = M(s,f)$ for each $s \in \cS $ and $f \in \cF$ with $\max_{s \in \cS} f(s) \leq S(f)$. Such $X^s$ and $Y_f$ are called a \emph{PSD factorization of size $d$}.
\end{theorem}
\begin{proof}
First, we prove that if $\cM$ has such a size $d$ relaxation, then $M$ has a factorization of size at most $d$. Let $\{A_i, b_i\}$ be the constraints of the SDP, $X^s$ be the feasible solutions, and $w^f$ be the affine objective functions. We assume that there exists an $X$ such that $A_i \cdot X = b_i$ and $X_i \succ 0$ is strictly feasible. Otherwise, the feasible region lies entirely on a face of $\psd{d}$, which itself is a PSD cone of smaller dimension, and we could take an SDP relaxation of smaller size. For an $f \in \cF$ such that $\max_{s \in \cS} f(s) \leq S(f)$, let $w^f(X)$ have maximum $w^*$ on the feasible region of the SDP. By \prettyref{lem:duality}, there exists $(y_f, Y_f)$ such that 
\[w^* - w_f(X) = Y_f \cdot X - \sum_i y_i(A_i \cdot X - b_i).\]
Substituting $X^s$ and adding $\mu_f = C(f) - w^* \geq 0$ (the inequality follows because the SDP relaxation achieves approximation $(C,S)$), we get
\[M(s,f) = Y_f \cdot X^s + \mu_f.\]

For the other direction, let $w^f(X) = C(f) - \mu_f - Y_f \cdot X$, let the $X^s$ be the feasible solutions, and let the constraints be empty, so the SDP is simply $X \succeq 0$. Then for any $f$ satisfying the soundness guarantee, 
\[\max_{X \succeq 0} w^f(X) = C(f) - \mu_f - \min_{X \succeq 0} Y_f \cdot X = C(f) - \mu_f \leq C(f).\]
Clearly the $X^s$ are feasible because they are PSD, and so we have constructed a $(C,S)$-approximate SDP relaxation.
\end{proof}

\section{Polynomial Formulations, Theta Body and SOS SDP Relaxations}\label{sec:polyforms}
In this section we first define what a polynomial formulation for a combinatorial optimization problem $\cM$ is, and then use that formulation to derive two families of SDP relaxations for $\cM$: The Theta Body and Sum-of-Squares relaxations. In \prettyref{cha:symmetric_sdps} we will see a few problems for which these relaxations achieve the best approximation guarantees of any symmetric SDP relaxation. 
\begin{definition}
A \emph{degree $d$-polynomial formulation on $n$ variables} for a combinatorial optimization problem $\cM = (\cS, \cF)$ is three sets of degree $d$ polynomials $\cP,\cQ,\cO \subseteq \R[x_1,\dots,x_n]_d$ and a bijection $\phi: \cS \leftrightarrow V(\cP) \cap H(\cQ)$ such that for each $f \in \cF$, there exists a polynomial $o^f \in \cO$ with $o^f(\phi(s)) = f(s)$. We call $\cP$ the equality constraints, $\cQ$ the inequality constraints, and $\cO$ the objective polynomials. The polynomial formulation is called \emph{boolean} if $V(\cP) \cap H(\cQ) \subseteq \{0,1\}^n$. 
\end{definition}
\begin{example}
\textsc{Matching} on $n$ vertices has a degree two polynomial formulation on $\binom{n}{2}$ variables. Let 
\[\cP = \matching.\] 
For a matching $M$, let $(\chi_M)_{ij} = 1$ if $(i,j) \in M$ and $0$ otherwise. Then clearly $\phi(M) = \chi_M$ is a bijection, and it is easily verified that every $\chi_M \in V(\cP)$. Finally, for an objective function $f_E(M) = |M \cap E|$, we define $p^{f_E}(x) = \sum_{ij: (i,j) \in E} x_{ij}$. 
\end{example}
A polynomial formulation for $\cM$ defines a polynomial optimization problem: given input $p^f$, maximize $p^f(x)$ subject to $p(x) = 0$ for each $p \in \cP$ and $q(x) \geq 0$ for each $q \in \cQ$. This optimization problem is equivalent to solving the combinatorial optimization $\cM$. 

In \prettyref{sec:polyproofs} we discussed how polynomial optimization problems could sometimes be solved by searching for PC$_>$ proofs of nonnegativity. Furthermore, these proofs can be found using semidefinite programming. It should come as no surprise then that, having rephrased $\cM$ as a polynomial optimization problem, there are SDP relaxations based on finding certificates of nonnegativity. The first, called the Theta Body relaxation, we only consider when the polynomial formulation has no inequality constraints. It finds a certificate of nonnegativity for $r(x)$ which is an SOS polynomial $s(x)$ together with a polynomial $g \in \gen{\cP}$ such that $r(x) = s(x) + g(x)$. 

Let $(\cP, \cO, \phi)$ be a degree-$d$ polynomial formulation for $\cM$. Recall that every polynomial $p$ of degree at most $2d$ has a $d \times d$ matrix of coefficients $\widehat{p}$ such that $p(\alpha) = \widehat{p} \cdot \mbasis(\alpha)(\mbasis(\alpha))^T$.
\begin{definition}
For $D \geq d$, the \emph{$D$th Theta-Body Relaxation} of $(\cP,\cO,\phi)$ is an SDP relaxation for $\cM = (\cS, \cF)$ consisting of: 
\begin{itemize}
\item Semidefinite program: $\widehat{p} \cdot X = 0$ for every $p \in \gen{\cP}$ of degree at most $2D$, $\widehat{1} \cdot X = 1$, and $X \succeq 0$.
\item Feasible solutions: $X^s = \Mbasis(\phi(s))(\Mbasis(\phi(s)))^T$.
\item Affine objective functions: $w^f(X) = \widehat{o^f} \cdot X$.
\end{itemize}
\end{definition}
This definition of the Theta Body Relaxation makes it obvious that it is an SDP relaxation for $\cM$, but we will frequently find it more convenient to work with the dual SDP. Working with the dual exposes the connection between SDP relaxations and polynomial proof systems.
\begin{lemma}
The dual of the Theta Body SDP Relaxation with objective function $\widehat{o^f}$ is $\min c$ subject to $c - o^f(x)$ is $2D$-SOS modulo $\gen{\cP}$.
\end{lemma}
\begin{proof}
The dual is
\[\begin{tabular}{ll} $\min y_1$ & subject to \\ $\widehat{1}y_1 - \widehat{o} = \widehat{s} + \sum_{p \in \gen{\cP}} \widehat{p}y_p$ \\ $\widehat{s} \succeq 0$\end{tabular}.\]
The constraint of the dual is a constraint on matrices, but we can also think of it as a constraint on degree $2D$ polynomials via the map $\widehat{p} \leftrightarrow p$. Because $\widehat{s} \succeq 0$, $\widehat{s}$ is a sum-of-squares polynomial. Thus this constraint is equivalent to asking that the polynomial $c - o(x)$ be $2D$-SOS modulo $\gen{\cP}$.
\end{proof}

Part of the certificate of the Theta Body is a polynomial $g \in \gen{\cP}$. However, because it does not find a proof that $g \in \gen{\cP}$ the Theta Body can be a bit unwieldy to work with. Its primal has a constraint for every $g \in \gen{\cP}$. Ideally we would only need constraints for polynomials in $\cP$. We can instead ask for a certificate which is an SOS polynomial $s$, a polynomial $g$, and a certificate that $g \in \gen{\cP}$. More generally, when $\cQ \neq \emptyset$, we can write an SDP to find a PC$_>$ proof of nonnegativity as a certificate. These are the Lasserre or SOS relaxations. 
\begin{definition}
Let $(\cP, \cQ, \cO, \phi)$ be a degree-$d$ polynomial formulation for $\cM = (\cS, \cF)$, and let $\cQ = \{q_1, \dots, q_k\}$. For $D \geq d$, the \emph{$D$th Lasserre Relaxation} or \emph{$D$th Sum-of-Squares Relaxation (SoS)} is an SDP relaxation for $\cM$ consisting of:
\begin{itemize}
\item Semidefinite program: $X = \diag{X_1, X_{q_1}, X_{q_2}, \dots, X_{q_k}}$. We include the constraints $(X_{q_i})_{UV} = X_1 \cdot \widehat{q_ix_Ux_V}$. This means if $X_1 = \widehat{p}$ for some polynomial $p$, then $X_{q_i} = \widehat{q_ip}$. For every $p \in \cP$ and polynomial $\lambda$ such that $\lambda p$ has degree at most $2d$, we have the constraint $\widehat{\lambda p} \cdot X_1 = 0$. Finally, we have $\widehat{1} \cdot X_1 = 1$ and $X \succeq 0$.
\item Feasible solutions: Let $X_0^s = \Mbasis(\phi(s))(\Mbasis(\phi(s)))^T$ and $X_i^s = X_0^s q_i(s)$. Then let
\[X^s = \diag{X_0^s, X_1^s,\dots,X_k^s}.\]
\item Affine objective functions: $w^o(X) = \widehat{o} \cdot X_1$.
\end{itemize}
\end{definition}
Once again, we will find it much more convenient to work with the dual to make the connections to polynomial proof systems more explicit.
\begin{lemma}
The dual of the degree $2D$ Sum-of-Squares SDP Relaxation with objective function $\widehat{o}$ is $\min c$ subject to $c - o(x)$ has a degree $2D$ PC$_>$ proof of nonnegativity from $\cP$ and $\cQ$.
\end{lemma}
\begin{proof}
The dual of the SOS relaxation is $\min y_1$ subject to
\begin{align*}
\diag{\widehat{1}y_1,0,\dots,0} - \diag{\widehat{o},0,\dots,0} = \widehat{s} &+ \sum_{\substack{p \in \gen{\cP} \\ \lambda}} \diag{\widehat{\lambda p},0,\dots,0} y_{\lambda p} +\\
&+ \sum_{\substack{i \in [k] \\ U,V}} \diag{\widehat{q_ix_Ux_V},0,\dots,0,\widehat{-x_Ux_V},0,\dots,0}y_{iUV}
\end{align*}
where in the last sum the second nonzero diagonal is in the $i$th place. Clearly $\widehat{s}$ must be block-diagonal since everything else is block-diagonal. Furthermore, we know that the $i$th block of $\widehat{s}$ is equal to $\sum_{UV} \widehat{x_Ux_V}y_{iUV}$ since the LHS is zero in every block but the first. Since $S \succeq 0$, this block must also be PSD, and thus must correspond to a sum-of-squares polynomial $s_i$. The constraint on the first block is then 
\[\widehat{1}y_1 - \widehat{o} = \widehat{s_1} + \sum_{\substack{p \in \gen{\cP} \\ \lambda}} \widehat{\lambda p} y_{\lambda p} + \sum_{i=1}^k \widehat{s_i q_i}.\] 
As a constraint on polynomials, this simply reads that $y_1 - o(x)$ must have a degree $2D$ PC$_>$ proof of nonnegativity from $\cP$ and $\cQ$. 
\end{proof}

The $D$th Theta Body and SOS relaxations are each relaxations of size $N = \binom{n+D-1}{D}$, since their feasible solutions have one coordinate for each monomial up to total degree $D$. For both hierarchies, it is clear that by projecting onto the coordinates up to degree $D' < D$, the feasible region of the $D'$th relaxation is contained in the feasible region of the $D'$th relaxation. Thus if the $D$th relaxation achieves a $(C,S)$-approximation, so does the $D$th. Furthermore, sometimes if we go high enough in the hierarchy we get a perfect relaxation.
\begin{lemma}
If the polynomial formulation is boolean, then the $n$th Theta Body and $n$th SOS relaxation are both exact. 
\end{lemma}
\begin{proof}
Follows immediately from \prettyref{fact:boolSOS}
\end{proof}

\subsection{Relations Between Theta Body and Lasserre Relaxations}
Here we compare and contrast the two different relaxations. Let $(\cP, \cO, \phi)$ be a polynomial formulation for $\cM = (\cS, \cF)$. 
\begin{lemma}
If the $D$th Lasserre relaxation achieves $(C,S)$ approximation of $\cM$, then the $D$th Theta Body relaxation does as well. 
\end{lemma}
\begin{proof}
The lemma follows immediately by noticing that any degree $2D$ PC$_>$ proof of nonnegativity from $\cP$ for a polynomial $r(x)$ implies that $r(x)$ is $2D$-SOS modulo $\gen{\cP}$. 
\end{proof}
We can also prove a partial converse in some cases:
\begin{proposition}\label{prop:tb_to_sos}
If $\cP$ is $k$-effective and the $D$th Theta Body relaxation achieves $(C,S)$ approximation of $\cM$, then the $kD$th Lasserre relaxation does as well.
\end{proposition}
\begin{proof}
Because the Theta Body relaxation is a $(C,S)$-approximation of $\cM$, we have that, for every $f \in \cF$ with $\max f \leq S(f)$, there exists a number $c_f \leq C(f)$ such that $c_f - o^f(x)$ is $2D$-SOS modulo $\gen{\cP}$. In other words, there is a polynomial identity $c_f - o^f(x) = s(x) + g(x)$, where $s$ is an SOS polynomial and $g \in \gen{\cP}$. Because $\cP$ is $k$-effective, $g$ has a degree $2kD$ derivation from $\cP$, so we have a polynomial identity
\[c_f - o^f(x) = s(x) + \sum_{p \in \cP} \lambda_p(x)p(x).\]
This implies that $(c_f, s, \lambda_p)$ are feasible solutions for the $kD$th Lasserre relaxation, and since $c_f \leq C(f)$, it achieves a $(C,S)$-approximation.
\end{proof}
\begin{example}
For \textsc{CSP}, $\cP = \{x_i^2 - 1 | i \in [n]\}$, and by \prettyref{cor:csp-effective}, $\cP$ is $1$-effective. Thus the $D$th Theta Body and Lasserre Relaxations are identical in this case.
\end{example}
\prettyref{prop:tb_to_sos} allows us to translate results about the Theta Body relaxations to Lasserre relaxations. In particular, in \prettyref{sec:symmetric_sdps} we will see how easy it is prove that Theta Body relaxations are optimal among symmetric relaxations of a given size. If the constraints are effective, this allows us to conclude that Lasserre relaxations which are not too much larger are just as good. This allows us to lower bound the size of \emph{any} symmetric SDP relaxation by finding lower bounds for Lasserre relaxations. 

\section{Symmetric Relaxations}
Often, combinatorial optimization problems will have underlying symmetries in their solution spaces. This extra structure allows us to prove things about these problems more easily. In this section we define what we mean by symmetric versions of all the problem formulations we have presented above. First, we recall some basic group theory.
\begin{definition}
Let $G$ be a group and $X$ be a set. We say \emph{$G$ acts on $X$} if there is a map $\phi: G \rightarrow (X \rightarrow X)$ satisfying $\phi(1)(x) = x$ and $\phi(g_1)(\phi(g_2)(x)) = \phi(g_1g_2)(x)$. In practice we omit the $\phi$ and simply write $gx$ for $\phi(g)(x)$.
\end{definition}
\begin{definition}
Let $G$ act on $X$. Then $\text{Orbit}(x) = \{y | \exists g: g(x) = y\}$ is called the \emph{orbit} of $x$, and $\text{Stab}(x) = \{g | g(x) = x\}$ is called the \emph{stabilizer} of $x$. 
\end{definition}
\begin{fact}[Orbit-Stabilizer Theorem]
Let $G$ act on $X$. Then $|G: \text{Stab}(x)| = |\text{Orbit}(x)|$. 
\end{fact}

Optimization problems often have natural symmetries, which we can represent by the existence of a group action.
\begin{definition}
A combinatorial optimization problem $\cM = (\cS, \cF)$ is $G$-symmetric if there are actions of $G$ on $\cS$ and $\cF$ such that $gf(gs) = f(s)$. 
\end{definition}

\begin{definition}
An SDP relaxation $(\{X^s\},\{(A_i,b_i)\},\{w^f\})$ for a $G$-symmetric problem $\cM$ is $G$-symmetric if there is an action of $G$ on $\psd{d}$ such that $gX^s = X^{gs}$ for every $s$, and $w^{gf}(gX) = w^f(X)$, and for every $i$, $gA_i = A_j$ for some other $j$. We say the relaxation is $G$-coordinate-symmetric if the action of $G$ is by permutation of the coordinates, in other words $G$ has an action on $[d]$ and $(gX)_{ij} = X_{gi,gj}$.
\end{definition}

\begin{definition}
A polynomial formulation $(\cP,\cQ,\cO,\phi)$ on $n$ variables for a $G$-symmetric problem $\cM$ is $G$-symmetric if there is an action of $G$ on $[n]$, extending to an action on polynomials simply by extending $gx_i = x_{gi}$ multiplicatively and linearly, such that $gp \in \cP$ for each $p \in \cP$, $gq \in \cQ$ for each $q \in \cQ$, and $go \in \cO$ for each $o \in \cO$. Note that this implies that $G$ fixes $\gen{\cP}$ as well, and that the natural action of $G$ on $\R^n$ also fixes $V(\cP) \cap H(\cQ)$. Finally, we require $g\phi(s) = \phi(gs)$.
\end{definition}

\begin{lemma}
If a $G$-symmetric problem $\cM$ has a $G$-symmetric polynomial formulation, then the Theta Body and SOS SDP relaxations are $G$-coordinate-symmetric.
\end{lemma}
\begin{proof}
The $D$th Theta Body and SOS SDP relaxations have one coordinate for every monomial up to degree $D$, so $N = \binom{n+D-1}{D}$. We index the coordinates by these monomials. We define an action of $G$ on $\psd{N}$ which permutes $[N]$ simply by its action on monomials inherited by the $G$-symmetric polynomial formulation. Under this action, for any polynomial $p$, we have $\hat{p}\cdot (gX) = \widehat{g^{-1}p} \cdot X$. Since $\gen{\cP},\cQ,\cO$ are fixed by $G$ and $\widehat{g^{-1}1} = \hat{1}$, it is clear that the constraints and objective functions of both the Theta Body and SOS relaxations are invariant under $G$. The feasible solutions are also invariant:
\[gX^s = \mbasis(g\phi(s))(\mbasis(g\phi(s)))^T = \mbasis(\phi(gs))(\mbasis(\phi(gs)))^T = X^{gs},\]
concluding the proof.
\end{proof}

When we have a $G$-symmetric combinatorial optimization problem, it makes some sense to write symmetric SDP relaxations for it.
The structure and symmetries of the problem are reflected in the relaxation, and it can often be interpreted more easily. 
In \prettyref{cha:symmetric_sdps} we will see that the Theta Body relaxation achieves the best approximation among any symmetric SDP of a similar size for \textsc{Matching}.
However, there are examples where asymmetric SDP relaxations achieve better approximation, even when the underlying problem is symmetric \cite{}. 