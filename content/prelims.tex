\chapter{Preliminaries}
In this chapter we define and discuss the basic mathematical concepts needed for this dissertation.
\section{Combinatorial Maximization Problems}
We define maximization problems here, but it is clear that the definition extends easily to minimization problems as well.
\begin{definition}
An \emph{combinatorial maximization problem} \(\cP = (\cS, \cF)\)
consists of a finite set
\(\cS\) of feasible solutions and a set \(\cF\) of nonnegative
objective functions. An algorithm for such a problem takes as input an $f \in \cF$ and computes
$\max_{s \in \cS} f(s)$.

We can also generalize to approximate solutions: Given two functions \(\tilde{C}, \tilde{S} \colon \cF \rightarrow \R\)
called approximation guarantees, we say
an algorithm \((\tilde{C}, \tilde{S})\)-approximately solves \(\cP\)
if given any \(f \in \cF\) with \(\max_{s \in \cS} f(s) \leq \tilde{S}(f)\) as input,
it computes \(\tilde{f}\in \R\) satisfying
\(\max_{s \in \cS} f(s) \leq \tilde{f} \leq \tilde{C}(f)\).
\end{definition}

We think of the functions $f \in \cF$ as defining the problem instances and the feasible solutions $s \in \cS$ as defining the combinatorial objects we are 
trying to maximize over. $\tilde{C}$ and $\tilde{S}$ can be thought of as the usual approximation parameters \emph{completeness} and \emph{soundness}. Any
algorithm which $(\tilde{C},\tilde{S})$-approximately solves $\cP$ can distinguish between when the input satisfies $\max_{s\in \cS} f(s) \leq \tilde{S}$ and when it satisfies $\max_{s \in\cS} f(s) \geq \tilde{C}$. Now we give a few concrete examples of combinatorial maximization problems:
\begin{example}
Recall that the Maximum Matching problem is, given a graph $G = (V,E)$, find a maximum set of disjoint edges. We can express this as a combinatorial optimization problem for each $n$ as follows: $K_n$ be the complete graph on $n$ vertices. The set of feasible solutions $\cS$ is the set of all maximum matchings on $K_n$. The objective functions will be indexed by edge subsets of $K_n$ and defined $f_E(M) = |E \cap M|$. It is clear that for a graph $G = (V, E)$ with $|V| = n$, the size of the maximum matching in $G$ is exactly $\max_{M \in \cS} f_E(M)$.
\end{example}
\begin{example}
The Traveling Salesperson Problem (TSP) is, given a set $X$ and a function $c: X\times X \rightarrow \R^+$, find a permutation $\pi$ of $X$ that minimizes the total cost of adjacent pairs in the permutation (including the first and last elements). This can be cast in this framework easily: the set of feasible solutions $\cS$ is the set of all permutations of $n$ elements. The objective functions are indexed by the function $c$ and can be written $f_c(\pi) = \sum_{i=1}^n c(\pi(i),\pi(i+1))$, where $n+1$ is taken to be $1$. TSP is a minimization problem rather than a maximization problem, so we ask for the algorithm to compute $\min_{\pi \in \cS} f(\pi)$ instead. 
\end{example}
There are many ways one can try to solve these combinatorial optimization problems. In this dissertation we will mostly be concerned with the method of Semidefinite Programming (SDP) relaxations, which has emerged recently as a powerful and widely applicable technique. 

\section{SDP Formulations for Optimization Problems}
Here we will define the concepts necessary for solving optimization problems using SDP.
\begin{definition}
A symmetric matrix $A \in \R^{n \times n}$ is called \emph{positive semidefinite} (PSD) if it meets any of the following equivalent criteria:
\begin{itemize}
\item Every eigenvalue of $A$ is non-negative,
\item $x^TAx \geq 0$ for every vector $x \in \R^n$,
\item $A = UU^T$ for some matrix $U$.
\end{itemize}
We write $A \succeq 0$ to denote that $A$ is PSD. 
\end{definition}
It is a well-known fact that the set of PSD matrices forms a cone in the set of symmetric matrices. We denote this cone with $\psd{d}$.
\begin{definition}
A \emph{Semidefinite Program} (SDP) is simply a convex optimization over an affine slice of the cone of positive semi-definite matrices. In other words, an SDP is
specified by a linear function $C: \psd{d} \rightarrow \R$ called the objective, and linear functions $A_i: \psd{d} \rightarrow \R$ and scalars $b_i$ 
collectively called the constraints. The SDP is then the optimization problem $\max C(X)$ subject to $X \in \psd{d}$ and $\forall i: A_i(X) = b_i$.
\end{definition}
SDPs can be solved in time polynomial in $d$ using the Ellipsoid Method \cite{}\footnote{}. Because of this, there has been a huge effort to rewrite many combinatorial
optimization problems as SDPs, see for example \cite{}. One of the main contributions of this dissertation is to prove some of the limits of this method, specifically
that the Maximum Matching problem cannot be expressed as a symmetric SDP (which we will define a bit later) of polynomial size. First, however, we must say what exactly we mean when we say "`rewrite as an SDP"'.
\begin{definition}
Let $\cP = (\cS, \cF)$ be a combinatorial maximization problem with approximation guarantees $(\tilde{C}, \tilde{S})$. Then a
  \emph{\((\tilde{C}, \tilde{S})\)-approximate SDP formulation of \(\cP\)}
  of size \(d\)
  consists of constraints $(A_i, b_i)$
  together with
  \begin{enumerate}
  \item \emph{Feasible solutions:}
    for each $s \in \cS$, there is an \(X^s \in \psd{d}\) with \(\forall i: A_i(X^s) = b_i\), i.e., the feasible region
    \(\Gset{X \in \psd{d}}{\forall i: A_i(X) = b_i}\)
    is a relaxation of \(\conv\face{X^s \mid s \in \cS}\),
  \item \emph{Objective functions:}
    an affine function \(w^f \colon \psd{d} \rightarrow \R\)
	satisfying
    \(w^f(X^s) = f(s)\) 
    for all \(f \in \cF\) with \(\max_{s \in \cS} f(s) \leq \tilde{S}(f)\) and 	all \(s \in \cS\),
    i.e., the linearizations are exact on solutions, and
  \item \emph{Achieving guarantee:}
    \(\max \face{ w^f(X) \mid \cA(X) = b, X \in \psd{d}} \leq \tilde{C}(f)\)
    for all \(f \in \cF\) with \(\max_{s \in \cS} f(s) \leq \tilde{S}(f)\).
  \end{enumerate}
	If $\tilde{C}(f) = \tilde{S}(f) = \max_{s\in \cS} f(s)$, we say the formulation is \emph{exact}.
\end{definition}
Given a $(\tilde{C}, \tilde{S})$-approximate SDP formulation for $\cP$, we can $(\tilde{C}, \tilde{S})$-approximately solve $\cP$ on input $f$ simply by solving
the SDP $\max w^f(X)$ subject to $X \in \psd{d}$ and $\forall i: A_i(X) = b_i$.

\begin{example}
The Matching Polytope is an exact formulation of the Maximum Matching problem. The Matching Polytope is defined as follows for each $n \in \Z$: Let $M$ be a maximum matching of the complete graph $K_n$, and let $\chi_M \in \R^{\binom{n}{2}}$ be the characteristic vector of this matching, i.e. $(\chi_M)_{ij} = 1$ iff $(i,j) \in M$. The matching polytope is $\conv\face{\chi_M | M \text{ a matching of $K_n$}}$. To embed this in the cone of PSD matrices rather than the positive orthant, we simply take $P_n = \conv\face{\diag{\chi_M} | M \text{ a matching of $K_n$}}$. Because $P_n$ is a polytope, there is another description $P_n = \{X: \forall i: A_i(X) \leq b_i\}$. To get the same form as in our definition, CLEAR THIS UP LATER

%This can be seen to be an SDP formulation by taking the feasible solutions to be $\chi_M$, the objective function $f_E$ can be expressed as $w^{f_E} = \sum_{ij \in E} x_{ij}$. The achievement guarantee is satisfied because $\max_{x \in P_n} w^{f_E}(x)$ must be achieved at a vertex, and any vertex of the polytope must be a $\chi_M$, so $\max_{x \in P_n} w^{f_E}(x) = w^{f_E}(\chi_{M^*}) = f_E(M^*) = \max_{M \in \cS} f_E(M)$. 
\end{example}

\section{Polynomial Formulations}
\begin{definition}
A \emph{degree $d$-polynomial formulation on $n$ variables} for a combinatorial optimization problem $\cM = (\cS, \cF)$ is three sets of degree $d$ polynomials $\cP,\cQ,\cO \subseteq \R[x_1,\dots,x_n]_d$ and a bijection $\phi: \cS \leftrightarrow V(\cP) \cap H(\cQ)$ such that for each $f \in \cF$, there exists a polynomial $o^f \in \cO$ with $o^f(\phi(s)) = f(s)$. We call $\cP$ the equality constraints, $\cQ$ the inequality constraints, and $\cO$ the objective polynomials. The polynomial formulation is called \emph{boolean} if $V(\cP) \cap H(\cQ) \subseteq \{0,1\}^n$. 
\end{definition}
\begin{example}
\textsc{Matching} on $n$ vertices has a degree two polynomial formulation on $\binom{n}{2}$ variables. Let $\cP = \{x_{ij}^2 - x_{ij} | i,j \in [n], i\neq j\} \cup \{\sum_j x_{ij} - 1 | i \in [n]\} \cup \{x_{ij}x_{ik} | i,j,k \in [n] \text{ all distinct.}\}$. For a matching $M$, let $\chi_M(x_{ij}) = 1$ if $(i,j) \in M$ and $0$ otherwise. Then clearly $\phi(M) = \chi_M$ is a bijection, and it is easily verified that every $\chi_M \in V(\cP)$. Finally, for an objective function $f_E(M) = |M \cap E|$, we define $p^{f_E}(x) = \sum_{ij: (i,j) \in E} x_{ij}$. 
\end{example}
A polynomial formulation for $\cM$ defines a polynomial optimization problem: given input $p^f$, maximize $p^f(x)$ subject to $p(x) = 0$ for each $p \in \cP$ and $q(x) \geq 0$ for each $q \in \cQ$. This optimization problem is equivalent to solving the combinatorial optimization $\cM$. 

TODO: SEGWAY. When the objective polynomials are affine, $\conv(V(\cP))$ is an SDP formulation for $\cM$. Unfortunately, the objective polynomials are not always affine, and even when they are, sometimes $\conv(V(\cP))$ has too many facets and so its size as an SDP formulation is too large for efficient optimization. This is the case for the above formulation for \textsc{Matching}. However, there are other ways one can create SDP formulations.

When the polynomial formulation has no inequality constraints, there is an interesting family of SDP relaxations. Let $(\cP, \cO, \phi)$ be a degree-$d$ polynomial formulation for $\cM$. Recall that every polynomial $p$ of degree at most $2d$ has a $d \times d$ matrix of coefficients $\widehat{p}$ such that $p(\alpha) = \widehat{p} \cdot \mbasis(\alpha)(\mbasis(\alpha))^T$.
\begin{definition}
For $D \geq d$, the \emph{$D$th Theta-Body Relaxation} of $(\cP,\cO,\phi)$ is an SDP relaxation for $\cM = (\cS, \cF)$ consisting of: 
\begin{itemize}
\item Semidefinite program: $\widehat{p} \cdot X = 0$ for every $p \in \gen{\cP}$ of degree at most $2D$, $\widehat{1} \cdot X = 1$, and $X \succeq 0$.
\item Feasible solutions: $X^s = \Mbasis(\phi(s))(\Mbasis(\phi(s)))^T$.
\item Affine objective functions: $w^o(X) = \widehat{o} \cdot X$.
\end{itemize}
\end{definition}
This definition of the Theta Body Relaxation makes it obvious that it is an SDP relaxation for $\cM$, but we will frequently find it more convenient to work with the dual SDP. Working with the dual exposes the connection between SDP relaxations and polynomial proof systems.
\begin{lemma}
The dual of the Theta Body SDP Relaxation with objective function $\widehat{o}$ is $\min c$ subject to $c - o(x)$ is $2D$-SOS modulo $\gen{\cP}$.
\end{lemma}
\begin{proof}
By TODO: CITE PRELIM, the dual is
\[\begin{tabular}{ll} $\min y_1$ & subject to \\ $\widehat{1}y_1 - \widehat{o} = \widehat{s} + \sum_{p \in \gen{\cP}} \widehat{p}y_p$ \\ $\widehat{s} \succeq 0$\end{tabular}.\]
The constraint of the dual is a constraint on matrices, but we can also think of it as a constraint on degree $2D$ polynomials via the map $\widehat{p} \leftrightarrow p$. Because $\widehat{s} \succeq 0$, $\widehat{s}$ is a sum-of-squares polynomial. Thus this constraint is equivalent to asking that the polynomial $c - o(x)$ be $2D$-SOS modulo $\gen{\cP}$.
\end{proof}

TODO: PROPERTIES OF THETA-BODY, LIKE CONVERGENCE AFTER N STEPS, AND OTHER STUFF. THETA BODY DEPENDS ONLY ON $\gen{\cP}$ AND NOT $\cP$.

When the polynomial formulation contains inequality constraints, there is a different hierarchy of SDP relaxations one can define. Its ability to handle inequality constraints has made it an extremely popular choice for approximately solving many combinatorial or polynomial optimization problems.
\begin{definition}
Let $(\cP, \cQ, \cO, \phi)$ be a degree-$d$ polynomial formulation for $\cM = (\cS, \cF)$, and let $\cQ = \{q_1, \dots, q_k\}$. For $D \geq d$, the \emph{$D$th Lasserre Relaxation} or \emph{$D$th Sum-of-Squares Relaxation (SoS)} is an SDP relaxation for $\cM$ consisting of:
\begin{itemize}
\item Semidefinite program: $X = \diag{X_1, X_{q_1}, X_{q_2}, \dots, X_{q_k}}$. We include the constraints $(X_{q_i})_{UV} = X_1 \cdot \widehat{q_ix_Ux_V}$. This means if $X_1 = \widehat{p}$ for some polynomial $p$, then $X_{q_i} = \widehat{q_ip}$. For every $p \in \cP$ and polynomial $\lambda$ such that $\lambda p$ has degree at most $2d$, we have the constraint $\widehat{\lambda p} \cdot X_1 = 0$. Finally, we have $\widehat{1} \cdot X_1 = 1$ and $X \succeq 0$.
\item Feasible solutions: Let $X_0^s = \Mbasis(\phi(s))(\Mbasis(\phi(s)))^T$ and $X_i^s = X_0^s q_i(s)$. Then let
\[X^s = \diag{X_0^s, X_1^s,\dots,X_k^s}.\]
\item Affine objective functions: $w^o(X) = \widehat{o} \cdot X_1$.
\end{itemize}
\end{definition}
Once again, we will find it much more convenient to work with the dual to make the connections to polynomial proof systems more explicit.
\begin{lemma}
The dual of the degree $2D$ Sum-of-Squares SDP Relaxation with objective function $\widehat{o}$ is $\min c$ subject to $c - o(x)$ has a degree $2D$ PC$_>$ proof of nonnegativity from $\cP$ and $\cQ$.
\end{lemma}
\begin{proof}
By TODO: CITE PRELIM, the dual of the SoS relaxation is $\min y_1$ subject to
\begin{align*}
\diag{\widehat{1}y_1,0,\dots,0} - \diag{\widehat{o},0,\dots,0} = \widehat{s} &+ \sum_{\substack{p \in \gen{\cP} \\ \lambda}} \diag{\widehat{\lambda p},0,\dots,0} y_{\lambda p} +\\
&+ \sum_{\substack{i \in [k] \\ U,V}} \diag{\widehat{q_ix_Ux_V},0,\dots,0,\widehat{-x_Ux_V},0,\dots,0}y_{iUV}
\end{align*}
where in the last sum the second nonzero diagonal is in the $i$th place. Clearly $\widehat{s}$ must be block-diagonal since everything else is block-diagonal. Furthermore, we know that the $i$th block of $\widehat{s}$ is equal to $\sum_{UV} \widehat{x_Ux_V}y_{iUV}$ since the LHS is zero in every block but the first. Since $S \succeq 0$, this block must also be PSD, and thus must correspond to a sum-of-squares polynomial $s_i$. The constraint on the first block is then 
\[\widehat{1}y_1 - \widehat{o} = \widehat{s_1} + \sum_{\substack{p \in \gen{\cP} \\ \lambda}} \widehat{\lambda p} y_{\lambda p} + \sum_{i=1}^k \widehat{s_i q_i}.\] 
As a constraint on polynomials, this simply reads that $y_1 - o(x)$ must have a degree $2D$ PC$_>$ proof of nonnegativity from $\cP$ and $\cQ$. 
\end{proof}

\subsection{Relations Between Theta Body and Lasserre Relaxations}
Here we compare and contrast the two different relaxations. Let $(\cP, \cO, \phi)$ be a polynomial formulation for $\cM = (\cS, \cF)$. 
\begin{lemma}
If the $D$th Lasserre relaxation achieves $(C,S)$ approximation of $\cM$, then the $D$th Theta Body relaxation does as well. 
\end{lemma}
\begin{proof}
The lemma follows immediately by noticing that any degree $2D$ PC$_>$ proof of nonnegativity from $\cP$ for a polynomial $r(x)$ implies that $r(x)$ is $2D$-SOS modulo $\gen{\cP}$. 
\end{proof}
We can also prove a partial converse in some cases:
\begin{proposition}\label{prop:tb_to_sos}
If $\cP$ is $k$-effective and the $D$th Theta Body relaxation achieves $(C,S)$ approximation of $\cM$, then the $kD$th Lasserre relaxation does as well.
\end{proposition}
\begin{proof}
Because the Theta Body relaxation is a $(C,S)$-approximation of $\cM$, we have that, for every $f \in \cF$ with $\max f \leq S(f)$, there exists a number $c_f \leq C(f)$ such that $c_f - o^f(x)$ is $2D$-SOS modulo $\gen{\cP}$. In other words, there is a polynomial identity $c_f - o^f(x) = s(x) + g(x)$, where $s$ is an SOS polynomial and $g \in \gen{\cP}$. Because $\cP$ is $k$-effective, $g$ has a degree $2kD$ derivation from $\cP$, so we have a polynomial identity
\[c_f - o^f(x) = s(x) + \sum_{p \in \cP} \lambda_p(x)p(x).\]
This implies that $(c_f, s, \lambda_p)$ are feasible solutions for the $kD$th Lasserre relaxation, and since $c_f \leq C(f)$, it achieves a $(C,S)$-approximation.
\end{proof}
\begin{example}
For \textsc{CSP}, $\cP = \{x_i^2 - 1 | i \in [n]\}$, and by \prettyref{lem:TODO}, $\cP$ is $1$-effective. Thus the $D$th Theta Body and Lasserre Relaxations are identical in this case.
\end{example}
\prettyref{prop:tb_to_sos} allows us to translate results about the Theta Body relaxations to Lasserre relaxations. In particular, in \prettyref{sec:symmetric_sdps} we will see how easy it is prove that Theta Body relaxations are optimal among symmetric relaxations of a given size. If the constraints are effective, this allows us to conclude that Lasserre relaxations which are not too much larger are just as good. This allows us to lower bound the size of \emph{any} symmetric SDP relaxation by finding lower bounds for Lasserre relaxations. 

\section{Polynomial Proof Systems}
A polynomial proof system is a set of rules through which one derives facts about potentially very complicated polynomials given base assumptions about simpler polynomials. For example, say our goal is to prove that some polynomial $r$ is zero on some set of points $S$. Maybe we already have a set of polynomials $\{p_i\}$ which we know are zero on $S$. One natural way of proving that $r$ is zero is to express $r$ as a combination of the polynomials $\cP = \{p_i\}$, i.e. find an identity $r = \sum_i \lambda_i \cdot p_i$ for some polynomial multipliers $\lambda_i$. EXPLAIN MORE LATER
\begin{definition}
Let $\cP$ be any set of polynomials in $\R[x_1,\dots,x_n]$, and let $S$ be any set of points in $\R^n$.
\begin{itemize}
\item We call $V(\cP) = \{x \in \R^n | \forall p \in \cP: p(x) = 0\}$ the \emph{real variety} of $\cP$. 
\item We call $I(S) = \{p \in R[x_1,\dots,x_n] | \forall x \in S: p(x) = 0\}$ the \emph{vanishing ideal} of $S$.
\item We denote $\gen{\cP} = \{q \in R[x_1,\dots,x_n]: \exists \lambda_p: q = \sum_{p \in \cP} \lambda_p \cdot p\}$ for the \emph{ideal generated by $\cP$}.
\end{itemize}
\end{definition}

\begin{definition}
Let $\cP = \{p_1, p_2, \dots, p_n\}$ be a set of polynomials. We say that \emph{$r$ is derived from $\cP$ in degree $d$} if there is a polynomial identity of the form
\[r(x) = \sum_{i=1}^n \lambda_i(x) \cdot p_i(x),\]
and $\max_i \deg(\lambda_i \cdot p_i) \leq d$. We often call this a Polynomial Calculus or PC proof.
\end{definition}
\begin{theorem}[Hilbert's Nullstellensatz]
Let $\cP$ be such that if $r^k \in \gen{\cP}$ for some $k \in \Z$, then $r \in \gen{\cP}$. Then $\gen{\cP} = I(V(\cP))$.
\end{theorem}
\begin{corollary}
If $V(\cP) = \emptyset$, then $\gen{\cP} = \R[x_1,\dots,x_n] = \gen{1}$.
\end{corollary}
Hilbert's Nullstellensatz states that the PC proof system is complete (as long as the axioms $\cP$ generate an ideal with the above property). In other words, \emph{any} polynomial $r$ that is zero on $V(\cP)$ is an element of $\gen{\cP}$, i.e. has a derivation from $\cP$. 


The PC proof system and Hilbert's Nullstellensatz give us powerful tools for trying to prove that a polynomial is zero on a certain set. However, this is not enough for our purposes. Something we would really like to be able to do is to prove that a polynomial is positive. TODO.
\begin{definition}
A polynomial $s(x) \in \R[x_1,\dots,x_n]$ is called a \emph{sum-of-squares (or SOS)} polynomial if $s(x) = \sum_i h_i^2(x)$ for some polynomials $h_i \in \R[x_1,\dots,x_n]$.
\end{definition}
\begin{definition}
Let $\cP$ and $\cQ$ be two sets of polynomials. A polynomial $r(x)$ is said to have a \emph{degree $d$ proof of nonnegativity from $\cP$ and $\cQ$} if there is a polynomial identity of the form
\[r(x) = s_0(x) + \sum_{q \in \cQ} s_q(x) \cdot q(x) + \sum_{p \in \cP} \lambda_p(x) \cdot p(x),\]
where $s_0$, and each $s_q$ are SOS polynomials. We often call this a PC$_>$ proof. 
\end{definition}