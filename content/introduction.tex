\chapter{Introduction}\label{cha:introduction}

\section{Combinatorial Optimization and Approximation}
Combinatorial optimization problems have been intensely studied by mathematicians and computer scientists for many years.
Here we mean any computational task which involves maximizing a function over some discrete set of feasible solutions.
The function to maximize is given as input to an algorithm, which attempts to find the feasible solution which achieves the best value. 
Here are a few examples of problems which will appear repeatedly throughout this thesis: 
\begin{example}
The \textsc{Matching} problem is, given a graph $G = (V,E)$, compute the size of the largest subset $F \subseteq E$ such that any two edges $e_1,e_2 \in F$ are disjoint.
\end{example}
\begin{example}
The \textsc{Traveling Salesperson}, or \textsc{TSP} problem is, given a set of points $X$ and a distance function $d: X \times X \rightarrow \R_+$, compute the least distance traveled by any tour which visits every point in $X$ exactly once and returning to the starting point.
\end{example}
\begin{example}
The \textsc{$c$-Balanced CSP} problem is, given boolean formulas $\phi_1,\dots,\phi_m$, compute the largest number of $\phi_1,\dots,\phi_m$ that can be simultaneously satisfied by an assignment with a $c$-fraction of variables assigned true.
\end{example}

Computer scientists initially began studying combinatorial optimization problems because they appear frequently in both practice and theory.
For example, \textsc{TSP} naturally arises when trying to plan school bus routes and \textsc{Matching} clearly shows up when trying to match medical school graduates to hospitals for residency. Unfortunately for the school bus driver, solving \textsc{TSP} has proven to be exceedingly difficult because optimizing such routes is \np-hard \cite{Karp1972}. Indeed, almost all combinatorial problems of interest are \np-hard (\textsc{Matching} is a notable exception), and are thus believed to be computationally intractable. The barrier of \np-hardness for these problems has been in place since the 1970s.

In an attempt to overcome this roadblock, the framework of approximation algorithms emerged a few years later in 1976 \cite{SG76}. Rather than trying to exactly solve \textsc{TSP} by finding the route that minimizes the distance traveled, an approximation algorithm attempts to find a route that is not too much longer than the minimum possible route. For example, maybe the algorithm finds a route that is guaranteed to be at most twice the length of the minimum route, even though the minimum route itself is impossible to efficiently compute. A wide variety of algorithmic techniques have been brought to bear on approximation problems. In this work we will focus on writing \emph{convex relaxations} for combinatorial problems in order to approximate them.

\section{Convex Relaxations}
A popular strategy in approximation algorithm design is to develop convex relaxations for combinatorial problems, as can be seen for example in \cite{GW95,VY99,ARV09,Li13}.
Since the solution space for combinatorial problems is discrete, we frequently know of no better maximization technique than to simply evaluate a function on every point in the space. However, if we embed the combinatorial solutions somehow in a continuous space and the combinatorial function as a continuous function $f$, we can enlarge the solution space to make it convex. The enlarged space is called the \emph{feasible region} of the convex relaxation. If we choose our feasible region carefully, then standard convex optimization techniques can be applied to optimize $f$ over it. Because the new solution space is larger than just the set of discrete solutions, the value we receive will be an overestimate of the true discrete maximum of $f$. We want the convex relaxation to be a \emph{good approximation} in the sense that this overestimate is not too far from the true maximum of $f$.

\begin{example}[Held-Karp relaxation for \textsc{TSP}]
Given an instance of \text{TSP}, i.e. distance function $d: [n] \times [n] \rightarrow \R$, for every tour $\tau$ (a cycle which visits each $i \in [n]$ exactly once), let $(\chi_\tau)_{ij} = 1$ if $\tau$ visits $j$ immediately after or before $i$. Each $\chi_\tau$ is an embedding of a tour $\tau$ in $\R^{\binom{n}{2}}$. Then
\[K = \left\{x \, \bigg{|} \, \forall S \subset [n]: \sum_{(i,j) \in S \times \overline{S}} x_{ij} \geq 2, \forall ij: 0 \leq x_{ij} \leq 1\right\}\]
and the function $f = \sum_{ij} x_{ij}$ is a convex relaxation for \textsc{TSP}. In fact, when $d$ is symmetric, $\min_K f$ is at least $2/3$ the true minimum.
\end{example}

Proving that a relaxation is a good approximation is usually highly non-trivial, and is frequently done by exhibiting a \emph{rounding scheme}. A rounding scheme is an  algorithm that takes a point in the relaxed body and maps it to one of the original feasible solutions for the combinatorial problem. Rounding schemes are designed so that they output a point with approximately the same value, i.e. within a multiplicative factor of $\rho$. This implies that minimizing over the relaxed body gives an answer that is within a factor of $\rho$ of minimizing over the discrete solutions. As an example, Christofides' approximation for \textsc{TSP} \cite{Chri76} can be interpreted as a rounding algorithm for the above relaxation which achieves an approximation factor of $3/2$.

In this thesis we will consider a particular kind of convex relaxation, called a semidefinite program (SDP). In an SDP, the enlarged convex body is the intersection of an affine plane with the cone of positive semidefinite (PSD) matrices, that is, the set of symmetric matrices which have all non-negative eigenvalues. The Ellipsoid Algorithm (a detailed history of which can be found in \cite{Akg84}) can be used to optimize a linear function over convex bodies in time polynomial in the dimension\footnote{Actually the runtime of the Ellipsoid Algorithm also depends polynomially on $\log R$, where $R$ is the radius of the smallest ball intersecting the feasible region of the SDP. This technical requirement is usually not an issue for most SDPs, but it will turn out to be important for studying the SOS hierarchy, which will see a bit later.} so long as there is an efficient procedure to find a separating hyperplane for points outside the body. If a matrix is not PSD, then it must have an eigenvector with a negative eigenvalue. This eigenvector forms a separating hyperplane, and since eigenvector computations can be performed efficiently, the Ellipsoid Algorithm can be used to efficiently optimize linear functions over the feasible regions of SDPs. 

SDPs are generalizations of linear programs (LPs), which are convex relaxations whose feasible regions are the intersection of an affine plane with the non-negative orthant. LPs have enjoyed extensive use in approximation algorithms (see \cite{WSbook11} for an in-depth discussion). Since the non-negative orthant can be obtained as a linear subspace of the PSD cone (the diagonal of the matrices), SDPs should be able to provide stronger approximation algorithms than LPs. 

SDPs first appeared in \cite{Lovasz79} as a method to study approximation of the \textsc{Independent Set} problem. The work of \cite{GW95} catapaulted SDPs to the cutting edge of approximation algorithms research when the authors wrote an SDP relaxation with a randomized rounding algorithm for the \textsc{Max Cut} problem, achieving the first non-trivial, polynomial-time approximation. We now know that this result separates SDPs from LPs, as \cite{CLRS16} implies that any LP relaxation achieving such approximation for \textsc{Max Cut} must be exponential size. In fact, the SDP of \cite{GW95} is so effective for this problem that it remains the best polynomial-time approximation for \textsc{Max Cut} we know, even decades years later. Since then SDPs have seen a huge amount of success in the approximation world for a wide variety of problems, including clustering \cite{PW07}, tensor decomposition \cite{TS15}, \textsc{Vertex Cover} \cite{Kar09}, \textsc{Sparsest Cut} \cite{ARV09}, graph coloring \cite{Chlam07}, and \emph{especially} constraint satisfaction problems (CSPs) \cite{FJ97,HZ99,CMM09}. In fact, if a complexity assumption called the Unique Games Conjecture \cite{Khot02} is true, then the work of Raghavendra \cite{Ragh08} implies that SDP relaxations provide optimal approximation algorithms for CSPs; to develop a better algorithm would prove \textbf{P}$ = $\np. 

The success of SDPs has prompted significant investigation into the limits of their power. For Boolean combinatorial problems, in principle one could write an SDP with an exponential number of variables that exactly solves the problem. However, such an SDP would not be of much use since even the Ellipsoid Algorithm would require an exponential amount of time to solve the SDP. The study of lower bounds for SDPs has thus been focused on proving that approximating a combinatorial problem requires an SDP with a large number of variables. This can be seen as a continuation of the work on LP lower bounds of Yannakakis \cite{Yann88}, in which he proved that the \textsc{Matching} problem has no symmetric LP relaxation of subexponential size. The symmetric requirement was finally dropped 25 years later by \cite{Roth14}, and more asymmetric lower bounds were given in \cite{FMPTHW15} for \textsc{TSP} and \cite{CLRS16} for CSPs. 
However, SDP relaxations are fairly new compared to LP relaxations, and there are significantly fewer examples of strong SDP lower bounds. The existence of 0/1 polytopes that require exponential-size exact SDP relaxations is proven in \cite{BDP15}. In \cite{FSP13} and \cite{LRST14} the authors provide exponential symmetric SDP lower bounds for CSPs, and \cite{LRS15} are able to drop the symmetry requirement, again for CSPs. 


%Even with a convex relaxation which is a good approximation, we still do not really have an approximation algorithm because we need a way to return an actual feasible combinatorial solution with good value. The second piece of the puzzle is called a rounding scheme, which is a function that takes one of the feasible points in the convex relaxation and takes it to one of the combinatorial solutions. The rounding scheme should be structured in such a way that the value of its output should not be too far from the value of its input. With this, we can round the value of the relaxation to a value for the combinatorial optimization problem. Rounding schemes are generally very nontrivial and developing them is a very interesting problem in its own right. The reader

\section{Sum-of-Squares Relaxations}
There is a particular family of SDP relaxations which has received a great deal of attention recently as a promising tool for approximation algorithms. Called the Sum-of-Squares (SOS) or Lasserre relaxations, they first appeared in \cite{Shor1987,Parr00,Las01} as a sequence of SDPs that eventually exactly converge to any 0/1 polytope. They have recently formed the foundation of algorithms for many different problems, ranging from tensor problems \cite{TS15,BKS15,HSS15,PS17} to Independent Set \cite{CS08}, Knapsack \cite{KMN10}, and \textsc{CSP}s and \textsc{TSP} \cite{RT12,LRST14}. There has even been hope among computer scientists that the SOS relaxations could represent a single, unified algorithm which achieves optimal approximation guarantees for many, seemingly unrelated problems \cite{BS14}. We give a brief description of the SOS relaxations here and give a more precise definition in \prettyref{sec:polyforms}.

We consider a polynomial embedding of a combinatorial optimization problem, i.e. there are sets of polynomials $\cP$ and $\cQ$ such that solving
\begin{align*}
&\max r(x) \\
\text{s.t. } &p(x) = 0, \forall p \in \cP \\
&q(x) \geq 0, \forall q \in \cQ
\end{align*}
is equivalent to solving the original combinatorial problem. This is not unusual, and indeed every combinatorial optimization problem has such an embedding. One way to solve such an optimization problem is to pick some $\theta$ and check if $\theta - r(x)$ is non-negative on the feasible points. If we can do this, then by binary search we can compute the maximum of $r$ quickly. But how can we check if $\theta - r(x)$ is non-negative? The SOS relaxations attempt to express $\theta - r(x)$ as a sum-of-squares polynomial, modulo the constraints of the problem. In other words, they try to find a polynomial identity of the form
\[\theta - r(x) = \sum_i s_i^2(x) + \sum_{q \in \cQ} \left(\sum_j s_{qj}^2(x)\right)q(x) + \sum_{p\in\cP} \lambda_p(x)p(x)\label{eq:intro_sos_proof}\]
for some polynomials $\{s_i\}, \{s_{qj}\}, \{\lambda_p\}$.
We call such an identity an SOS proof of non-negativity for $\theta - r(x)$.
If such an identity exists, then certainly $\theta - r(x)$ is non-negative on any $x$ satisfying the constraints. As general quadratic optimization is \np-hard, looking for \emph{any} such identity is intractable, so we consider relaxing the problem to checking for the existence of such an identity that uses only polynomials up to degree $2d$. The existence of a degree $2d$ identity turns out to be equivalent to the feasibility of a certain SDP of size $n^{O(d)}$ (see \prettyref{sec:polyforms} for specifics), which we call the \emph{degree-$d$} or \emph{$d$th SOS SDP}.

While the SOS relaxations have been popular and successful, they are still relatively new, and so our knowledge about them is far from complete.
There are even very basic questions about them for which we do not know the answer.
In particular, we do not even know when we can solve the SOS relaxations in polynomial time!
Because the $d$th SOS relaxation is a semidefinite program of size $n^{O(d)}$, it is often claimed that any degree $d$ proof can be found in time polynomial in $n^{O(d)}$ via the Ellipsoid algorithm. However, this claim was debunked very recently by Ryan O'Donnell in \cite{ODon16}.
He noted that complications could arise if every proof of non-negativity involves polynomials with extremely large coefficients, and furthermore, he gave an explicit example showing that it is possible for this to occur.
%The SOS SDP is usually solved using the Ellipsoid Algorithm, which runs in time polynomial in $\log R$, where $R$ is the radius of the smallest ball containing the feasible region of the SDP.
%If every proof involves coefficients of doubly-exponential size, then the Ellipsoid Algorithm will run in exponential time.
%O'Donnell even provides an example of a set of polynomial constraints and a polynomial whose degree two proofs of non-negativity all \emph{must} contain coefficients of doubly exponential size, proving that there are some examples where we cannot solve the SOS relaxations in polynomial time.
%However, his example only held up to degree two, and in particular there were degree four proofs with small coefficients.
%On the positive side, O'Donnell noted that for the SOS relaxation which \emph{only} has the boolean constraints $\pcsp = \csp$, any proof can be taken to have small coefficients.
%Furthermore, he conjectured that any SOS relaxation which has constraints containing the boolean constraints can be solved efficiently by the Ellipsoid Algirthm.
Resolving this issue is of paramount importance, as the SOS relaxations lie at the heart of so many approximation algorithms.
In this dissertation, we continue this line of work with some positive and negative results discussed in \prettyref{sec:intro_contrib}.

Another open area of research is investigating the true power of the SOS relaxations.
Since we know SOS relaxations provide good approximation for so many computational problems, it is natural to continue to apply them to new or different problems.
This is a worthy pursuit, but not one that will be explored in this work.
An alternative approach would be to try to identify for which problems the SOS relaxations \emph{do not} provide good approximations.
For any Boolean optimization problem, if $d$ is large enough, then the $d$th SOS relaxation will solve the problem exactly.
However, if $d$ is too large, then the SDP will have a super-polynomial number of variables, so that even the Ellipsoid Algorithm cannot solve it in polynomial time.
Thus as for general SDP lower bounds, it is common to rephrase this question by giving a lower bound on the degree of the SOS relaxation required to achieve a good enough approximation.
The degree-$d$ SOS relaxation is size $n^{O(d)}$, so if $d$ is super-constant then the size of the SDP is super-polynomial.
This area of research has been much more fruitful than general SDP lower bounds, as the SOS relaxations are concrete objects which are more easily reasoned about. 
In \cite{Gri01}, Grigoriev gives a linear degree lower bound against the \textsc{Matching} problem. A sequence of results \cite{MPW15, DM15, RS15, HKP15,BHKKMP16} all give lower bounds against the \textsc{Planted Clique} problem. SOS lower bounds for \textsc{Densest $k$-Subgraph} are given in \cite{BCVGZ12}. Different \textsc{CSP} problems are considered in \cite{Sch08,RS09, GMT09,Tul09,LRS15}.
Proving more lower bounds is also a noble goal, but this thesis will focus on a slightly different evaluation of the effectiveness of the SOS relaxations.


Rather than evaluating the SOS relaxations by how good an approximation they achieve in the absolute sense, we will be evaluating them relative to other SDPs.
In particular, we will explore whether or not there exist other SDPs which perform better than the SOS relaxations.
Sometimes the answer is no; The SOS relaxations provide the best approximation among SDPs of a comparable size for CSPs \cite{LRST14,LRS15}. We will explore the slightly more restricted setting of \cite{LRST14}, where the other SDP relaxations we measure against must be symmetric in some sense, i.e. respect the natural symmetries of the corresponding combinatorial optimization problem (see \prettyref{sec:symmetric-defs} for details). 
%In this setting, we will show that the SOS relaxation is optimal for a few different problems, including \textsc{Matching} and \textsc{TSP}. This implies an exponential symmetric SDP lower bound for \textsc{Matching}, thanks to an SOS lower bound due to \cite{Gri01}.

\section{Polynomial Ideal Membership and Effective Derivations}

In order to study the SOS relaxations, in this dissertation we use as a technical tool polynomial proof systems and the existence of low-degree polynomial proofs.
Here we introduce a bit of background on these tools.
The polynomial ideal membership problem is the following computational task:
Given a set of polynomials $\cP = \{p_1,\dots,p_n\}$ and a polynomial $r$, we want to determine if $r$ is in the ideal generated by $\cP$ or not, denoted $\gen{\cP}$.
This problem was first studied by Hilbert \cite{Hilbert1893}, and has applications in solving polynomial systems \cite{CLO07} and polynomial identity testing \cite{AM10}. 
The theory of Gr\"obner bases \cite{Buch65} originated as a method to solve the membership problem. 
Unfortunately, the membership problem is \expspace-hard to solve in general \cite{MM82,Huynh1985}.
Luckily this will not impede us too much, since we will be studying this problem for the very special instances that correspond to common combinatorial optimization problems.

The membership problem is easily solvable if there exist low-degree proofs of membership for the ideal $\gen{\cP}$. 
Note that $r \in \gen{\cP}$ if and only if there exists a polynomial identity
\[r(x) = \sum_{p \in \cP} \lambda_p(x) p(x)\]
for some polynomials $\{\lambda_p \mid p \in \cP\}$. We call such a polynomial identity a \emph{derivation} or \emph{proof of membership} for $r$ from $\cP$.
If we had an \emph{a priori} bound on the degree required for this identity, we could simply solve a system of linear equations to determine the coefficients of each $\lambda_p$. The Effective Nullstellensatz \cite{Hermann1926} tells us that we can take $d \leq (\deg r)^{2^{|\cP|}}$. This bound is not terribly useful, because we would need to solve an enormous linear system. This is unavoidable in general because of the \expspace-hardness, but in specific cases we could hope for a better bound on $d$. 
In particular, the polynomial ideals that arise from combinatorial optimization problems frequently have nice properties that make them much more reasonable than arbitrary polynomial ideals. For example, these ideals are often Boolean (and thus have finite solution spaces) and highly symmetric. In these cases, we could hope for a much better degree bound. 

This problem has been studied in \cite{BIKPP94,BT98,Gri98,BGIP01}, mostly in the context of lower bounds. In these works the problem is referred to as the degree of \emph{Nullstellensatz proofs} of membership for $r$. In this work we will continue to study this problem, however we will be mostly interested in upper bounds on the required degree. We will be able to use the existence of low-degree Nullstellensatz proofs for combinatorial ideals to study the SOS relaxations. 



\section{Contribution of Thesis}\label{sec:intro_contrib}

In the first part of this thesis, to set the stage for analyzing the SOS relaxations, we give upper bounds on the required degree for Nullstellensatz proofs for many ideals arising from a number of combinatorial problems, including \textsc{Matching}, \textsc{TSP}, and \textsc{Balanced CSP}. In particular, we prove that if $\cP$ is a set of polynomials corresponding to \textsc{Matching}, \textsc{TSP}, or \textsc{Balanced CSP} and $r \in \gen{\cP}$, then $r$ has a Nullstellensatz proof degree at most $kr$ for $k\leq 3$. This implies that the polynomial ideal membership problem for these ideals has a polynomial-time solution. We achieve results for so many different ideals because we develop a meta-strategy which exploits the symmetries present in the underlying combinatorial optimization problems in order to give good upper bounds.

Recall that the SOS relaxation runs in polynomial time so long as the bit complexity of the SOS proof it is trying to find is polynomially bounded. 
The second part of the thesis is devoted to studying the problem of bit complexity in SOS proofs. 
Our main contribution is to show that SOS proofs for \textsc{Planted Clique}, \textsc{Max CSP}, \textsc{TSP}, \textsc{Bisection}, and some others can be taken to have polynomial bit complexity. This implies that SOS relaxations for these problems do indeed run in polynomial time, patching a potential problem with several known approximation algorithms. We prove this result by giving a set of criteria for the constraints $\cP$ and $\cQ$, one of which is that any polynomial $r \in \gen{\cP}$ has Nullstellensatz proofs of bounded degree. This partially motivates our study of Nullstellensatz proofs. 
On the negative side, we provide an example of a set of polynomials $\cP$ \emph{containing Boolean constraints} and a polynomial $r$ which has a degree-two SOS proof of non-negativity, but every SOS proof up until degree $\Omega(\sqrt{n})$ requires coefficients of size roughly $2^{2^{\sqrt{n}}}$. This refutes a conjecture of \cite{ODon16} that simply containing Boolean constraints suffices for polynomial bit complexity. 

The final part of the thesis investigates the power of the SOS relaxations, especially for the \textsc{Matching} problem. In particular, we prove the SDP version of Yannakakis' LP lower bound for \textsc{Matching}: The \textsc{Matching} problem has no subexponential-size symmetric SDP achieving a non-trivial approximation. We prove this by extending the techniques of \cite{LRST14} to \textsc{Matching} and show that the degree-$d$ SOS relaxation achieves the best approximation of any symmetric SDP  
of about the same size. Together with the SOS lower bound of Grigoriev \cite{Gri01}, this gives us the exponential lower bound. We prove a similar SOS optimality result for \textsc{TSP} and \textsc{Balanced CSP}, but there are no currently known SOS lower bounds for these problems. 

\section{Organization of Thesis}

\prettyref{cha:prelims} will contain preliminary and background discussion on mathematical concepts needed. We will precisely define many of the objects discussed in this introduction, including combinatorial optimization problems, SDP relaxations, and the SOS relaxations themselves. In \prettyref{cha:effective_derivations} we will discuss low-degree proofs of membership and compile a (non-exhaustive) list of combinatorial optimization problems which admit such proofs. In \prettyref{cha:bit_complexity}, we discuss the bit complexity of SOS proofs, and show how low-degree proofs can be used to prove the existence of SOS proofs with small bit complexity. In \prettyref{cha:symmetric_sdps} we discuss the optimality of the SOS relaxations, and show how this implies an exponential size lower bound for approximating \textsc{Matching}. Finally, in \prettyref{cha:future_work} we discuss a few open problems continuing the lines of research of this thesis.


%If for every $r \in \gen{\cP}$, $r$ has a derivation from $\cP$ of degree $k\deg r$, then we say $\cP$ is $k$-effective. In this case the linear system required to find a polynomial identity would involve only $n^{O(k \deg r)}$ equations, and since the size of the input required to specify $r$ is $n^{O(\deg r)}$, this is polynomial in the input size for constant $k$. 

%To see one way ideal membership relates to SOS relaxations, imagine we are given a set of polynomials $\{s_i\}$ and a polynomial $q$ which are claimed to prove that $r$ is non-negative on the zeros of $\cP$ via the polynomial identity
%\begin{align}
%r = \sum_i s_i^2 + q,
%\end{align}
%and the claim that $q \in \gen{\cP}$. Clearly this implies that $r$ is non-negative on the zeros of $\cP$, but how do we verify such a proof? With the polynomials in hand, checking the identity is trivial, but we must also check that $q \in \gen{\cP}$, i.e. solve the membership problem.
%This exact situation will come up later in the dissertation, due to a different SDP relaxation, called the Theta Body \cite{GPT10}, that certifies non-negativity of polynomials via low-degree certificates of this type.
%For the SOS relaxation to perform as well as this SDP, it needs to be able to solve the ideal membership problem, at least for low-degree polynomials.

%Clearly if $q \in \gen{\cP}$, there is a set of polynomials $\{\lambda_p \mid p \in \cP\}$ such that
%\[q = \sum_{q \in \cP} \lambda_p \cdot p.\]
%Moreover, if $\max_i \deg(\lambda_p \cdot p) \leq k\deg p$, then any Theta Body proof of degree $d$ can be translated into an SOS proof of degree $kd$.
%This implies that the SOS relaxations perform as well as the Theta Body relaxations, although one has to take the size of the SOS relaxation as large as $n^{O(kd)}$ instead of $n^{O(d)}$.
%This is particularly interesting because it is often not difficult to prove that the Theta Body relaxation is optimal among symmetric SDP relaxations of a comparable size (it is implicit in the work of \cite{LRST14}), which we will see more examples of in \prettyref{cha:symmetric_sdps}. 

%The above is only one example of the potential applications of low-degree proofs for the membership problem for various $\cP$. 
%In \prettyref{cha:bit_complexity} we will also use the existence of low-degree proofs of membership to study the bit complexity of SOS proofs. 
%Because of these applications, we wish to know when a set $\cP$ admits low-degree proofs, thereby solving the membership problem.
