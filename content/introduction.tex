\chapter{Introduction}\label{cha:introduction}

\section{Combinatorial Optimization and Approximation}
Combinatorial optimization problems have been intensely studied by mathematicians and computer scientists for many years.
Here we mean any computational task which involves maximizing a function over some discrete set of feasible solutions.
The function to maximize is given as input to an algorithm, which attempts to find the feasible solution which achieves the best value. 
Here are a few examples of problems which will appear repeatedly throughout this thesis: 
\begin{example}
The \textsc{Matching} problem is, given a graph $G = (V,E)$, compute the size of the largest subset $F \subseteq E$ such that any two edges $e_1,e_2 \in F$ are disjoint.
\end{example}
\begin{example}
The \textsc{Traveling Salesperson}, or \textsc{TSP} problem is, given a set of points $X$ and a distance function $d: X \times X \rightarrow \R_+$, compute the least distance traveled by any tour which visits every point in $X$ exactly once and returning to the starting point.
\end{example}
\begin{example}
The \textsc{$c$-Balanced CSP} problem is, given boolean formulas $\phi_1,\dots,\phi_m$, compute the largest number of $\phi_1,\dots,\phi_m$ that can be simultaneously satisfied by an assignment with a $c$-fraction of variables assigned true.
\end{example}

Computer scientists initially began studying combinatorial optimization problems because they appear frequently in both practice and theory.
For example, \textsc{TSP} naturally arises when trying to plan school bus routes and \textsc{Matching} clearly shows up when trying to match medical school graduates to hospitals for residency. Unfortunately for the the school bus driver, solving \textsc{TSP} has proven to be exceedingly difficult because \textsc{TSP} is \np-hard\cite{Karp1972}. Indeed, almost all combinatorial problems of interest are \np-hard (\textsc{Matching} is a notable exception), and are thus believed to be computationally intractable. The barrier of \np-hardness for these problems has been in place since the 1970s.

In an attempt to overcome this roadblock, the framework of approximation algorithms emerged a few years later in 1976 \cite{SG76}. Rather than trying to exactly solve \textsc{TSP} by finding the route that minimizes the distance traveled, an approximation algorithm attempts to find a route that is not too much longer than the minimum possible route. For example, maybe the algorithm finds a route that is guaranteed to be at most twice the length of the minimum route, even though the minimum route itself is impossible to efficiently compute. A wide variety of algorithmic techniques have been brought to bear on approximation problems. In this work we will focus on writing \emph{convex relaxations} for combinatorial problems in order to approximate them.

\section{Convex Relaxations}
A popular strategy in approximation algorithm design is to develop convex relaxations for combinatorial problems, as can be seen for example in \cite{GW95,VY99,ARV09,Li13}.
Since the solution space for combinatorial problems is discrete, we frequently know of no better maximization technique than to simply evaluate a function on every point in the space. However, if we embed the combinatorial solutions somehow in a continuous space and the combinatorial function as a continuous function $f$, we can enlarge the solution space to make it convex. The enlarged space is called the \emph{feasible region} of the convex relaxation. If we choose our feasible region carefully, then standard convex optimization techniques can be applied to optimize $f$ over it. Because the new solution space is larger than just the set of discrete solutions, the value we receive will be an overestimate of the true discrete maximum of $f$. We want the convex relaxation to be a \emph{good approximation} in the sense that this overestimate is not too far from the true maximum of $f$.

\begin{example}[Held-Karp relaxation for \textsc{TSP}]
Given an instance of \text{TSP}, i.e. distance function $d: [n] \times [n] \rightarrow \R$, for every tour $\tau$ (a cycle which visits each $i \in [n]$ exactly once), let $(\chi_\tau)_{ij} = 1$ if $\tau$ visits $j$ immediately after $i$. Each $\chi_\tau$ is an embedding of a tour $\tau$ in $\R^{\binom{n}{2}}$. Then
\[K = \left\{x \, \bigg{|} \, \forall S \subset [n]: \sum_{(i,j) \in S \times \overline{S}} x_{ij} \geq 2, \forall ij: 0 \leq x_{ij} \leq 1\right\}\]
and the function $f = \sum_{ij} x_{ij}$ is a convex relaxation for \textsc{TSP}. In fact, when $d$ is symmetric, $\min_K f$ is at least $2/3$ the true minimum.
\end{example}

Proving that a relaxation is a good approximation is usually highly nontrivial, and is frequently done by exhibiting a \emph{rounding scheme}. A rounding scheme is an  algorithm that takes a point in the relaxed body and maps it to one of the original feasible solutions for the combinatorial problem. Rounding schemes are designed so that they output a point with approximately the same value, i.e. within a multiplicative factor of $\rho$. This implies that minimizing over the relaxed body gives an answer that is within a factor of $\rho$ of minimizing over the discrete solutions. As an example, Christofides' approximation for \textsc{TSP} \cite{Chri76} can be interpreted as a rounding algorithm for the above relaxation which achieves an approximation factor of $3/2$.

In this thesis we will consider a particular kind of convex relaxation, called a semidefinite program (SDP). In an SDP, the enlarged convex body is the intersection of an affine plane with the cone of positive semidefinite (PSD) matrices, that is, the set of symmetric matrices which have all non-negative eigenvalues. The Ellipsoid Algorithm (a detailed history of which can be found in \cite{Akg84}) can be used to optimize a linear function over convex bodies in time polynomial in the dimension\footnote{Actually the runtime of the Ellipsoid Algorithm also depends polynomially on $\log R$, where $R$ is the radius of the smallest ball intersecting the feasible region of the SDP. This technical requirement is usually not an issue for most SDPs, but it will turn out to be important for studying the SOS hierarchy, which will see a bit later} so long as there is an efficient procedure to find a separating hyperplane for points outside the body. If a matrix is not PSD, then it must have an eigenvector with a negative eigenvalue. This eigenvector forms a separating hyperplane, and since eigenvector computations can be done efficiently, the Ellipsoid Algorithm can be used to efficiently optimize linear functions over the feasible regions of SDPs. 

SDPs are generalizations of linear programs (LPs), which are convex relaxations whose feasible regions are the intersection of an affine plane with the non-negative orthant. LPs have enjoyed extensive use in approximation algorithms (see \cite{WSbook11} for an in-depth discussion). Since the non-negative orthant can be obtained as a linear subspace of the PSD cone (the diagonal of the matrices), SDPs should be able to provide stronger approximation algorithms than LPs. 

SDPs first appeared in \cite{Lovasz79} as a method to study approximation of the \textsc{Independent Set} problem. The work of \cite{GW95} catapaulted SDPs to the cutting edge of approximation algorithms research when the authors wrote an SDP relaxation with a randomized rounding algorithm for the \textsc{Max Cut} problem, achieving the first nontrivial, polynomial-time approximation. We now know that this result separates SDPs from LPs, as \cite{CLRS16} implies that any LP relaxation achieving such approximation for \textsc{Max Cut} must be exponential size. In fact, the SDP of \cite{GW95} is so effective for this problem that it remains the best polynomial-time approximation for \textsc{Max Cut} we know, even decades years later. Since then SDPs have seen a huge amount of success in the approximation world for a wide variety of problems, including clustering \cite{PW07}, tensor decomposition \cite{TS15}, \textsc{Vertex Cover} \cite{Kar09}, \textsc{Sparsest Cut} \cite{ARV09}, graph coloring \cite{Chlam07}, and \emph{especially} constraint satisfaction problems (CSPs) \cite{FJ97,HZ99,CMM09}. In fact, if a complexity assumption called the Unique Games Conjecture \cite{Khot02} is true, then the work of Raghavendra \cite{Ragh08} implies that SDP relaxations provide optimal approximation algorithms for CSPs; to develop a better algorithm would prove \bf{P}$ = $\bf{NP}. 

The success of SDPs has prompted significant investigation into the limits of their power. For Boolean combinatorial problems, in principle one could write an SDP with an exponential number of variables that exactly solves the problem. However, such an SDP would not be of much use since even the Ellipsoid Algorithm would require an exponential amount of time to solve the SDP. The study of lower bounds for SDPs has thus been focused on proving that approximating a combinatorial problem requires an SDP with a large number of variables. This can be seen as a continuation of the LP lower bounds of Yannakakis \cite{Yann88}. Yannakakis proved that the \textsc{Matching} problem had no symmetric LP relaxation of subexponential size. The symmetric requirement was finally dropped 25 years later by \cite{Roth14}, and more asymmetric lower bounds were given in \cite{FMPTHW15} for \textsc{TSP} and \cite{CLRS15} for CSPs. 
These are not an exhaustive list of references, and there are many more examples.
However, SDP relaxations are fairly new compared to LP relaxations, and there are significantly fewer examples of strong SDP lower bounds. In \cite{FSP13} and \cite{LRST14} the authors provide exponential symmetric SDP lower bounds for CSPs, and \cite{LRS15} are able to drop the symmetry requirement, again for CSPs.

The reader can find examples of these approaches in \cite{RS09, BPZ15, LRS15, BCVVZ12, BDP15}.

%Even with a convex relaxation which is a good approximation, we still do not really have an approximation algorithm because we need a way to return an actual feasible combinatorial solution with good value. The second piece of the puzzle is called a rounding scheme, which is a function that takes one of the feasible points in the convex relaxation and takes it to one of the combinatorial solutions. The rounding scheme should be structured in such a way that the value of its output should not be too far from the value of its input. With this, we can round the value of the relaxation to a value for the combinatorial optimization problem. Rounding schemes are generally very nontrivial and developing them is a very interesting problem in its own right. The reader

\section{Sum-of-Squares Relaxations}
There is a particular family of SDP relaxations which has received a great deal of attention recently as a promising tool for approximation algorithms. Called the Sum-of-Squares (SOS) or Lasserre relaxations, they first appeared in \cite{Parr00,Las01} as a sequence of SDPs that eventually exactly converge to any 0/1 polytope. They have recently formed the foundation of algorithms for many different problems, ranging from tensor problems \cite{TS15,BKS15,HSS15,PS17} to independent set \cite{CS08}, knapsack \cite{KMN10}, and \textsc{CSP}s and \textsc{TSP} \cite{RT12,LRST14}. There has even been hope among computer scientists that the SOS relaxations could represent a single, unified algorithm which achieves optimal approximation guarantees for many, seemingly unrelated problems \cite{BS14}. We give a brief description of the SOS relaxations here and give a more precise definition in \prettyref{sec:polyforms}.

We consider a polynomial embedding of a combinatorial optimization problem, i.e. there are sets of polynomials $\cP$ and $\cQ$ such that solving
\begin{align*}
&\max r(x) \\
\text{s.t. } &p(x) = 0, \forall p \in \cP \\
&q(x) \geq 0, \forall q \in \cQ
\end{align*}
is equivalent to solving the original combinatorial problem. This is not unusual, and indeed every combinatorial optimization problem has such an embedding. One way to solve such an optimization problem is to pick some $\theta$ and check if $\theta - r(x)$ is nonnegative on the feasible points. If we can do this, then by binary search we can compute the maximum of $r$ quickly. But how can we check if $\theta - r(x)$ is nonnegative? The SOS relaxations attempt to express $\theta - r(x)$ as a sum-of-squares polynomial, modulo the constraints of the problem. In other words, they try to find a polynomial identity of the form
\[\theta - r(x) = \sum_i s_i^2(x) + \sum_{q \in \cQ} \left(\sum_j s_{qj}^2(x)\right)q(x) + \sum_{p\in\cP} \lambda_p(x)p(x)\label{eq:intro_sos_proof}\]
for some polynomials $\{s_i\}, \{s_{qj}\}, \{\lambda_p\}$.
We call such an identity an SOS proof of nonnegativity for $\theta - r(x)$.
If such an identity exists, then certainly $\theta - r(x)$ is nonnegative on any $x$ satisfying the constraints. Looking for \emph{any} such identity would be intractable, so the $d$th SOS relaxation checks for the existence of such an identity that uses only polynomials up to degree $2d$. The existence of a degree $2d$ identity is then equivalent to the feasibility of a certain SDP of size $n^{O(d)}$ (see \prettyref{sec:polyforms} for specifics), which we call the $d$th SOS relaxation.

While the SOS relaxations have been popular and successful, they are still relatively new, and so our knowledge about them is far from complete.
There are even very simple questions for which we do not know the answer.
In particular, we do not even know when we can solve the SOS relaxations in polynomial time!
Because the $d$th SOS relaxation is a semidefinite program of size $n^{O(d)}$, it has been very common to claim that any degree $d$ proof can be found in time polynomial in $n^{O(d)}$ via the Ellipsoid algorithm. However, this claim was debunked very recently by Ryan O'Donnell in \cite{ODon16}.
He noted that complications can arise when every proof of non-negativity involves polynomials with extremely large coefficients, and gave an example to this effect.
%The SOS SDP is usually solved using the Ellipsoid Algorithm, which runs in time polynomial in $\log R$, where $R$ is the radius of the smallest ball containing the feasible region of the SDP.
%If every proof involves coefficients of doubly-exponential size, then the Ellipsoid Algorithm will run in exponential time.
%O'Donnell even provides an example of a set of polynomial constraints and a polynomial whose degree two proofs of nonnegativity all \emph{must} contain coefficients of doubly exponential size, proving that there are some examples where we cannot solve the SOS relaxations in polynomial time.
%However, his example only held up to degree two, and in particular there were degree four proofs with small coefficients.
%On the positive side, O'Donnell noted that for the SOS relaxation which \emph{only} has the boolean constraints $\pcsp = \csp$, any proof can be taken to have small coefficients.
%Furthermore, he conjectured that any SOS relaxation which has constraints containing the boolean constraints can be solved efficiently by the Ellipsoid Algirthm.
Resolving this issue is of paramount importance, as the SOS relaxations lie at the heart of so many approximation algorithms.
In this dissertation, we continue this line of work with some positive and negative results discussed in \prettyref{sec:intro_contrib}.

Another open area of research is investigating the true power of the SOS relaxations.
Since we know SOS relaxations provide good approximation for so many computational problems, it is natural to continue to apply them to new or different problems.
This is a worthy pursuit, but not one that will be explored in this work.
An alternative approach would be to try to identify for which problems the SOS relaxations \emph{do not} provide good approximations.
For any Boolean optimization problem, an SOS relaxation of large enough size exactly solves it.
However, the SDP might be so large that, even given small bit complexity, we cannot solve it in polynomial time.
It is common to rephrase this question by giving a lower bound on the size of an SOS relaxation required to achieve a good enough approximation.
For a few examples, \cite{Gri01} gives an exponential lower bound against the \textsc{Matching} problem. A sequence of results \cite{MPW15, DM15, RS15, HKP15,BHKKMP16} all give lower bounds against the \textsc{Planted Clique} problem. SOS lower bounds for \textsc{Densest $k$-Subgraph} are given in \cite{BCVGZ12}. Different \textsc{CSP} problems are considered in \cite{Sch08,GMT09,Tul09,LRS15}.
This too is a noble goal, but this thesis will focus on a slightly different evaluation of the effectiveness of the SOS relaxations.

Rather than evaluating the SOS relaxations by how good an approximation they achieve in the absolute sense, we will be evaluating them relative to other SDPs.
In particular, we will explore whether or not there exist other SDPs which perform better than the SOS relaxations.
There have been a few results showing that the answer is no for some combinatorial problems; the SOS relaxations provide the best approximation among SDPs of a comparable size for CSPs \cite{LRST14,LRS15}. We will explore the slightly more restricted setting of \cite{LRST14}, where the other SDP relaxations we consider must be symmetric in some sense, i.e. respect the natural symmetries of the corresponding combinatorial optimization problem (see \prettyref{sec:symmetric-defs} for details). In this setting, we will show that the SOS relaxation is optimal for a few different problems, including \textsc{Matching} and \textsc{TSP}. This implies an exponential symmetric SDP lower bound for \textsc{Matching}, thanks to an SOS lower bound due to \cite{Gri01}.

\section{Polynomial Ideal Membership}

In order to study the SOS relaxations, in this dissertation we use as a technical tool polynomial proof systems and the existence of low-degree polynomial proofs.
Here we introduce some of the background for these tools, and give an example showing their connection to the SOS relaxations.
The polynomial ideal membership problem is the following computational task:
Given a set of polynomials $\cP = \{p_1,\dots,p_n\}$ and a polynomial $p$, we want to determine if $p$ is in the ideal generated by $\cP$ or not, denoted $\gen{\cP}$.
This problem was first studied by Hilbert \cite{Hilbert1893}, and has applications in solving polynomial systems \cite{CLO07} and polynomial identity testing \cite{AM10}.
Solving the membership problem is \expspace-hard in general \cite{MM82,Huynh1985}, but fortunately we will be studying this problem for the very special instances that correspond to common combinatorial optimization problems and its applications to the SOS relaxations.

To see one way ideal membership relates to SOS relaxations, imagine we are given a set of polynomials $\{s_i\}, q$ which are claimed to prove that $r$ is nonnegative on the zeros of $\cP$ via the polynomial identity
\begin{align}
r = \sum_i s_i^2 + q,
\end{align}
and the claim that $q \in \gen{\cP}$. Clearly this implies that $r$ is nonnegative on the zeros of $\cP$, but how do we verify such a proof? With the polynomials in hand, checking the identity is trivial, but we must also check that $q \in \gen{\cP}$, i.e. solve the membership problem.
This exact situation will come up later in the dissertation, due to a different SDP relaxation, called the Theta Body \cite{GPT10}, that certifies nonnegativity of polynomials via low-degree certificates of this type.
For the SOS relaxation to perform as well as this SDP, it needs to be able to solve the ideal membership problem, at least for low-degree polynomials.

Clearly if $q \in \gen{\cP}$, there is a set of polynomials $\{\lambda_p \mid p \in \cP\}$ such that
\[q = \sum_{q \in \cP} \lambda_p \cdot p.\]
Moreover, if $\max_i \deg(\lambda_p \cdot p) \leq k\deg p$, then any Theta Body proof of degree $d$ can be translated into an SOS proof of degree $kd$.
This implies that the SOS relaxations perform as well as the Theta Body relaxations, although one has to take the size of the SOS relaxation as large as $n^{O(kd)}$ instead of $n^{O(d)}$.
This is particularly interesting because it is often not difficult to prove that the Theta Body relaxation is optimal among symmetric SDP relaxations of a comparable size (it is implicit in the work of \cite{LRST14}), which we will see more examples of in \prettyref{cha:symmetric_sdps}. 

The above is only one example of the potential applications of low-degree proofs for the membership problem for various $\cP$. 
In \prettyref{cha:bit_complexity} we will also use the existence of low-degree proofs of membership to study the bit complexity of SOS proofs. 
Because of these applications, we wish to know when a set $\cP$ admits low-degree proofs, thereby solving the membership problem.

\section{Contribution of Thesis}\label{sec:intro_contrib}

In the first part of this thesis, to set the stage for analyzing the SOS relaxations, we develop a strategy to show that symmetric sets of polynomials $\cP$ admit low-degree proofs of membership.
We believe our strategy to be widely applicable for symmetric constraints, and show how to apply it to numerous examples, including \textsc{Matching}, \textsc{TSP}, and \textsc{Balanced CSP}.
Low-degree proofs imply that the ideal membership problem for those ideals has a polynomial-time solution.
Furthermore, we will use these low-degree proofs in subsequent chapters of this dissertation in order to study the effectiveness of the SOS relaxations on these combinatorial problems.

The second part of the thesis is devoted to studying the problem of bit complexity in SOS proofs. 
Recall that the SOS SDP is usually solved using the Ellipsoid Algorithm, which runs in time polynomial in $\log R$, where $R$ is the radius of the smallest ball intersecting the feasible region of the SDP.
If every proof involves coefficients of doubly-exponential size, then the Ellipsoid Algorithm will run in exponential time.
In \cite{ODon16}, Ryan O'Donnell provides an example of a set of polynomial constraints and a polynomial whose degree two proofs of nonnegativity all \emph{must} contain coefficients of doubly exponential size, proving that there are some examples where we cannot solve the SOS relaxations in polynomial time.
Furthermore, he conjectured that any SOS relaxation which has constraints containing the Boolean constraints $\{x_i^2 - x_i \mid i \in [n]\}$ can be solved efficiently by the Ellipsoid Algirthm.
We present some of the first further results in this area, both positive and negative. 
On the positive side, we give a set of checkable criteria, one of which is the existence of low-degree proofs of membership for $\cP$, for $\cP$ and $\cQ$ that is sufficient to imply that any SOS proofs from $\cP$ and $\cQ$ can be taken to have small bit complexity. Armed with our library of such combinatorial optimization problems, we show that the SOS relaxations run in polynomial time for many of their prior applications. This alleviates some of the concern following O'Donnell's result.
On the negative side, we strengthen O'Donnell's original example, and refute his hope that any set of constraints including Boolean constraints has all SOS proofs with small bit complexity.

The final part of the thesis contains proofs that the SOS relaxations achieve the best approximation compared to any other \emph{symmetric} SDP relaxations of a comparable size for a few different problems. In particular, we prove that the SOS relaxations are optimal among symmetric relaxations for \textsc{Matching}, \textsc{TSP}, and \textsc{Balanced CSP}. Because a lower bound for approximating \textsc{Matching} is already known for SOS relaxations, this enables us to prove an exponential lower bound against approximating \textsc{Matching} using any symmetric SDP.

\section{Organization of Thesis}

\prettyref{cha:prelims} will contain preliminary and background discussion on mathematical concepts needed. We will precisely define many of the objects discussed in this introduction, including combinatorial optimization problems, SDP relaxations, and the SOS relaxations themselves. In \prettyref{cha:effective_derivations} we will discuss low-degree proofs of membership and compile a (nonexhaustive) list of combinatorial optimization problems which admit such proofs. In \prettyref{cha:bit_complexity}, we discuss the bit complexity of SOS proofs, and show how low-degree proofs can be used to prove the existence of SOS proofs with small bit complexity. In \prettyref{cha:symmetric_sdps} we discuss the optimality of the SOS relaxations, and show how this implies an exponential size lower bound for approximating \textsc{Matching}. Finally, in \prettyref{cha:future_work} we discuss a few open problems continuing the lines of research of this thesis.
