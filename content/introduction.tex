\chapter{Introduction}\label{cha:introduction}
Combinatorial optimization problems have been intensely studied by mathematicians and computer scientists for many years.
Here we mean any computational task which involves maximizing a function over some discrete set of feasible solutions, with the admissible functions and solutions changing depending on what the task is. 
Here are a few examples of problems which will appear again and again throughout this thesis:
\begin{example}
The \textsc{Matching} problem is, given a graph $G = (V,E)$, compute the size of the largest subset $F \subseteq E$ such that any two edges $e_1,e_2 \in F$ are disjoint.
\end{example}
\begin{example}
The \textsc{Traveling Salesperson}, or \textsc{TSP} problem is, given a set of points $X$ and a distance function $d: X \times X \rightarrow \R_+$, compute the least distance incurred by visiting every point in $X$ exactly once and returning to the starting point. 
\end{example}
\begin{example}
The \textsc{$c$-Balanced CSP} problem is, given boolean formulas $\phi_1,\dots,\phi_m$, compute the largest number of $\phi_1,\dots,\phi_m$ that can be simultaneously satisfied by an assignment with exactly $c$ variables assigned true. 
\end{example}

Because the framework of combinatorial optimization problems is so general, such computational tasks appear frequently in both practice and theory.
For example, \textsc{TSP} naturally arises when trying to plan school bus routes and \textsc{Matching} clearly shows up when trying to match medical school graduates to hospitals. Unfortunately for the the school bus driver, solving \textsc{TSP} is computationally intractable because \textsc{TSP} is \np-complete\cite{Karp1972}. Indeed, almost all combinatorial optimization problems are \np-complete (\textsc{Matching} is a notable exception), and this well-established barrier has been in place since the 1970s. 

In an attempt to overcome the \np-complete barrier, the framework of approximation algorithms emerged a few years later \cite{SG76}. Rather than trying to exactly solve \textsc{TSP} by finding the route that minimizes the distance traveled, an approximation algorithm attempts to find a route that is not too much longer than the minimum possible route. For example, maybe the algorithm finds a route that is guaranteed to be at most twice the length of the minimum route, even though the minimum route itself is impossible to efficiently compute. A wide variety of algorithmic techniques have been brought to bear on approximation problems. In this work we will focus on writing \emph{convex relaxations} for combinatorial problems in order ato approximate them.

\section{Convex Relaxations}
A popular strategy in approximation algorithm design is to develop convex relaxations for combinatorial optimization problems, as can be seen for example in \cite{GW95,ARV09,Li13,VY99}. 
Because the feasible solution space for combinatorial problems is discrete, we frequently know of no better maximization technique than to simply evaluate a function on every point in the space. However, if we embed the combinatorial solutions somehow in $\R^n$ and the combinatorial function as a continuous $f: \R^n \rightarrow \R$, we can relax the solution space to a larger convex body. Then standard convex optimization techniques can be applied to optimize $f$ over this convex body. Because the body is larger, the value we receive will be an overestimate of the true maximum of $f$ over just the combinatorial solutions. We want the convex relaxation to be a \emph{good approximation} in the sense that this overestimate is not too far from the true maximum of $f$. 
\begin{example}[Held-Karp LP]
Given an instance of \text{TSP}, i.e. distance function $d: [n] \times [n] \rightarrow \R$, for every tour $\tau$ (a cycle which visits each $i \in [n]$ exactly once), let $(\chi_\tau)_{ij} = 1$ if $\tau$ visits $j$ immediately after $i$. Each $\chi_\tau$ is an embedding of a tour $\tau$ in $\R^{\binom{n}{2}}$. Then 
\[K = \left\{x \mid \forall S \subset [n]: \sum_{(i,j) \in S \times \overline{S}} x_{ij} \geq 2, \forall ij: 0 \leq x_{ij} \leq 1\right\}\]
and the function $f = \sum_{ij} x_{ij}$ is a convex relaxation for \textsc{TSP}. In fact, when $d$ is symmetric, $\min_K f$ is at least $2/3$ the true minimum. 
\end{example}

Proving that a relaxation is a good approximation is usually highly nontrivial, and is frequently done by exhibiting a \emph{rounding scheme}. A rounding scheme is a potentially randomized algorithm that takes a point in the relaxed body and maps it to one of the original feasible solutions for the combinatorial problem. Rounding schemes are designed so that it outputs a point with approximately the same value. This implies that minimizing over the relaxed body is approximately the same as minimizing over the combinatorial solutions. As an example, Christofides' approximation for \textsc{TSP} \cite{Chri76} can be interpreted as a rounding algorithm for the above relaxation.  

In this thesis we will consider a particular kind of convex relaxation, called a semidefinite program (SDP). In an SDP, the relaxed set is the intersection of an affine plane with the cone of positive semidefinite matrices. It is easy to tell when a matrix does not lie in this body (just compute its eigenvalues), so the Ellipsoid Algorithm (a detailed history of which can be found in \cite{Akg84}) can be used to efficiently optimize a linear function over this body. SDPs first appeared in \cite{Lovasz79} in the form of the Lovasz Theta function. The work of \cite{GW95} catapaulted SDPs to the cutting edge of research when the authors wrote an SDP relaxation with a randomized rounding algorithm for the \textsc{Max Cut} problem, achieving the first nontrivial approximation. Since then SDPs have seen a huge amount of success in the approximation world \cite{ARV09,TS15,PW07,Kar09,CMM09,Chlam07,FJ97,HZ99,Ragh08}, which of course has also prompted plenty of work on lower bounds against them \cite{BPZ15,LRS15,BCVVZ12,RS09,BDP15}. 

%Even with a convex relaxation which is a good approximation, we still do not really have an approximation algorithm because we need a way to return an actual feasible combinatorial solution with good value. The second piece of the puzzle is called a rounding scheme, which is a function that takes one of the feasible points in the convex relaxation and takes it to one of the combinatorial solutions. The rounding scheme should be structured in such a way that the value of its output should not be too far from the value of its input. With this, we can round the value of the relaxation to a value for the combinatorial optimization problem. Rounding schemes are generally very nontrivial and developing them is a very interesting problem in its own right. The reader 

\section{Sum-of-Squares Relaxations}
There is a particular family of SDP relaxations which has received a large amount of attention recently as a promising tool for approximation algorithms. Called the Sum-of-Squares (SOS) or Lasserre relaxations, they first appeared in \cite{Las01} as a sequence of SDPs that eventually exactly converge to any 0/1 polytope. They have recently formed the foundation of algorithms for many different problems, ranging from tensor problems \cite{TS15,PS17,BKS15,HSS15} to independent set \cite{CS08}, knapsack \cite{KMN10}, and \textsc{CSP} and \textsc{TSP} \cite{LRST14,RT12}. There have even been rumblings of hope that the SOS relaxations could represent a single, unified algorithm which achieves optimal approximation guarantees for many, seemingly unrelated problems \cite{BS14}. We give a brief description of the SOS relaxations here and give a more precise definition in \prettyref{sec:polyforms}. 

We consider a polynomial embedding of a combinatorial optimization problem, i.e. there are sets of polynomials $\cP$ and $\cQ$ such that solving
\begin{align*}
&\max r(x) \\
\text{s.t. } &p(x) = 0, \forall p \in \cP \\
&q(x) \geq 0, \forall q \in \cQ
\end{align*}
is equivalent to solving the original combinatorial problem. This is not unusual, and indeed every combinatorial optimization problem has such an embedding. One way to solve such an optimization problem is to pick some $\theta$ and check if $\theta - r(x)$ is nonnegative on the feasible points. If we can do this, then by binary search we can compute the maximum of $r$ quickly. But how can we check if $\theta - r(x)$ is nonnegative? The SOS relaxations attempt to express $\theta - r(x)$ as a sum-of-squares polynomial, modulo the constraints of the problem. In other words, they try to find a polynomial identity of the form
\[\theta - r(x) = \sum_i s_i^2(x) + \sum_{q \in \cQ} \left(\sum_j s_{qj}^2(x)\right)q(x) + \sum_{p\in\cP} \lambda_p(x)p(x)\label{eq:intro_sos_proof}\]
for some polynomials $\{s_i\}, \{s_{qj}\}, \{\lambda_p\}$. 
We call such an identity an SOS proof of nonnegativity for $\theta - r(x)$.
If such an identity exists, then certainly $\theta - r(x)$ is nonnegative on any $x$ satisfying the constraints. Looking for \emph{any} such identity would be intractable, so the $d$th SOS relaxation checks for the existence of such an identity that uses only polynomials up to degree $d$. The existence of a degree $d$ identity is then equivalent to the feasibility of a certain SDP of size $O(n^d)$ (see \prettyref{sec:polyforms} for specifics), which we call the $d$th SOS relaxation.

While the SOS relaxations have been very popular and very successful, they are still relatively new, and so our knowledge about them is far from complete. 
There are even very simple questions for which we do not know the answer.
In particular, we do not even know when we can solve the SOS relaxations in polynomial time! 
Because the $d$th SOS relaxation is a semidefinite program of size $O(n^d)$, it has been very common to claim that any degree $d$ proof can be found in time polynomial in $O(n^d)$ via the Ellipsoid algorithm. However, this claim was debunked very recently by Ryan O'Donnell in \cite{ODon16}. 
He noted that complications can arise when every proof of nonnegativity involves polynomials with extremely large coefficients.
The SOS SDP is usually solved using the Ellipsoid Algorithm, which runs in time polynomial in $\log R$, where $R$ is the radius of the smallest ball containing the feasible region of the SDP. 
If every proof involves coefficients of doubly-exponential size, then the Ellipsoid Algorithm will run in exponential time. 
O'Donnell even provides an example of a set of polynomial constraints and a polynomial whose degree two proofs of nonnegativity all \emph{must} contain coefficients of doubly exponential size, proving that there are some examples where we cannot solve the SOS relaxations in polynomial time.
However, his example only held up to degree two, and in particular there were degree four proofs with small coefficients. 
On the positive side, O'Donnell noted that for the SOS relaxation which \emph{only} has the boolean constraints $\pcsp = \csp$, any proof can be taken to have small coefficients. 
Furthermore, he conjectured that any SOS relaxation which has constraints containing the boolean constraints can be solved efficiently by the Ellipsoid Algirthm.
Expanding his work in this area is of paramount importance, as the SOS relaxations lie at the heart of so many approximation algorithms. 
In this dissertation, we continue this line of work with some positive and negative results discussed in \prettyref{sec:intro_contrib}.

%This phenomenon is not always an issue. 
%Many applications of the SOS relaxations have an analysis that explicitly constructs proofs of nonnegativity, which naturally do not involve doubly exponential coefficients. 
%However, it would be nice to have a criterion that tells us when the Ellipsoid Algorithm for the SOS SDP will run in polynomial time, without first finding the proofs themselves, which are often complicated and hard to construct.

Another open area of research is the true power of the SOS relaxations.
Since we know SOS relaxations provide good approximation for so many computational problems, it is natural to continue to apply them to new or different problems.
This is a worthy pursuit, but not one that will be explored in this work. 
A different approach would be to try to identify for which problems the SOS relaxations \emph{do not} provide good approximations.
For any Boolean optimization problem, an SOS relaxation of large enough size exactly solves it. 
However, the SDP might be so large that, even given small bit complexity, we cannot solve it in polynomial time. 
It is common to rephrase this question by giving a lower bound on the size of an SOS relaxation required to achieve a good enough approximation. 
There has been a good amount of work in this area. For a few examples, \cite{Gri01} gives an exponential lower bound against the \textsc{Matching} problem. A number of independent results \cite{RS15,MPW15,BHKKMP16} all give lower bounds against the \textsc{Planted Clique} problem. SOS lower bounds for \textsc{Densest $k$-Subgraph} are given in \cite{BCVGZ12}. Different \textsc{CSP} problems are considered in \cite{GMT09,Tul09,Sch08,LRS15}. 
This too is a noble goal, but this thesis will focus on a slightly different evaluation of the effectiveness of the SOS relaxations.

Rather than evaluating the SOS relaxations by how good an approximation they achieve in the absolute sense, we will be evaluating them relative to other SDPs. 
In particular, we will explore whether or not there exist other SDPs which perform better than the SOS relaxations.
There have been a few results showing that the answer is no for some combinatorial problems; the SOS relaxations provide the best approximation among SDPs of a comparable size for CSPs \cite{LRST14,LRS15}. We will explore the slightly more restricted setting of \cite{LRST14}, where the other SDP relaxations we consider must be symmetric in some sense, i.e. respect the natural symmetries of the corresponding combinatorial optimization problem. In this setting, we will show that the SOS relaxation is optimal for a few different problems, including \textsc{Matching} and \textsc{TSP}. Readers who have been paying attention will notice that this implies a general symmetric SDP lower bound for \textsc{Matching}, thanks to \cite{Gri01}. 

In order to study the SOS relaxations in these contexts, we use as a technical tool polynomial proof systems and the existence of low-degree polynomial proofs. 

\section{Polynomial Ideal Membership}

The polynomial ideal membership problem is the following computational task:
Given a set of polynomials $\cP = \{p_1,\dots,p_n\}$ and a polynomial $p$, we want to determine if $p$ is in the ideal generated by $\cP$ or not, denoted $\gen{\cP}$. 
This problem was first studied by Hilbert \cite{Hilbert1893}, and has applications in solving polynomial systems \cite{CLO07} and polynomial identity testing \cite{AM10}.
Unfortunately, in general solving the membership problem is \expspace-hard \cite{MM82,Huynh1985}. 
However, we will be studying this problem for the very special instances that correspond to common combinatorial optimization problems and its applications to the SOS relaxations. 

To see one way ideal membership relates to SOS relaxations, imagine we are given a set of polynomials $\{s_i\}, p$ which are claimed to form a slightly different kind of proof that $r$ is nonnegative on the zeros of $\cP$: the polynomial identity
\begin{align}
r = \sum_i s_i^2 + p,
\end{align}
and it is claimed that $p \in \gen{\cP}$. Clearly this implies that $r$ is nonnegative on the zeros of $\cP$, but how do we verify such a proof? Checking the identity is trivial, but we must also check that $p \in \gen{\cP}$.  
This exact situation will come up later in the dissertation, due to a different SDP relaxation, called the Theta Body \cite{GPT10}, that certifies nonnegativity of polynomials via low-degree certificates of this type. 
For the SOS relaxation to perform as well as this SDP, it needs to be able to solve the ideal membership problem, at least for low-degree polynomials. 

Clearly if $p \in \gen{\cP}$, there is a set of polynomials $\lambda_1, \dots, \lambda_n$ such that 
\[p = \sum_i \lambda_i p_i.\]
Moreover, if $\max_i \deg \lambda_ip_i \leq k\deg p$, then any Theta Body proof of degree $d$ can be translated into an SOS proof of degree $kd$. 
If every polynomial in $\gen{\cP}$ has such a set of polynomials, we say that $\cP$ is $k$-effective. 
If $\cP$ is $k$-effective, then that is enough to show that the SOS and Theta Body relaxations coincide (although you have to take the size of the SOS relaxation to be a factor of $k$ larger). 
This is particularly interesting because it is often not difficult to prove that the Theta Body relaxation is optimal among symmetric SDP relaxations of a comparable size (it is implicit in the work of \cite{LRST14}), which we will see more examples of in \prettyref{cha:symmetric_sdps}. In \prettyref{cha:bit_complexity} we will also see that $\cP$ being $k$-effective has consequences for the bit complexity of SOS proofs using the polynomials $\cP$.

\section{Contribution of Thesis}\label{sec:intro_contrib}

In the first part of this thesis, to set the stage for analyzing the SOS relaxations, we develop a strategy to show that symmetric sets of polynomials $\cP$ are $k$-effective for various constant $k$. 
We believe our strategy to be widely applicable for symmetric constraints, and show how to apply it to numerous examples, including \textsc{Matching}, \textsc{TSP}, and \textsc{Balanced CSP}. 
We collect a wide variety of problems which are $k$-effective, which implies that the ideal membership problem for those ideals has a polynomial-time solution. 

The second part of the thesis is devoted to studying the problem of bit complexity in SOS proofs. We present some of the first results in this area, both positive and negative. We give a set of checkable criteria, one of which is $k$-effectiveness, for $\cP$ and $\cQ$ that is sufficient to imply that any SOS proofs from $\cP$ and $\cQ$ can be taken to have small bit complexity. Armed with our library of $k$-effective combinatorial optimization problems, we show that the SOS relaxations run in polynomial time for many of their prior applications. This alleviates some of the worry following O'Donnell's result. 
On the negative side, we strengthen O'Donnell's original example, and refute his hope that any set of constraints including boolean constraints has all SOS proofs with small bit complexity. 
Furthermore, our criteria does have limitations, and in particular cannot be used to show that \emph{refutations}, that is, SOS proofs of $-1 \geq 0$ for infeasible systems of polynomial constraints, can be taken to have polynomial bit complexity.

The final part of the thesis contains proofs that the SOS relaxations achieve the best approximation compared to any other \emph{symmetric} SDP relaxations of a comparable size for a few different problems. In particular, we prove that the SOS relaxations are optimal for \textsc{Matching}, \textsc{TSP}, and \textsc{Balanced CSP}. Because a lower bound for approximating \textsc{Matching} is already known for SOS relaxations, this enables us to prove an exponential lower bound against approximating \textsc{Matching} using any symmetric SDP. 

\section{Organization of Thesis}

\prettyref{cha:prelims} will contain preliminary and background discussion on mathematical concepts needed for the contributions of the thesis. We will precisely define many of the objects discussed in this introduction, including combinatorial optimization problems, SDP relaxations, and the SOS relaxations themselves. In \prettyref{cha:effective_derivations} we will discuss how to prove $k$-effectiveness and compile a (nonexhaustive) list of combinatorial optimization problems whose constraints are $k$-effective. In \prettyref{cha:bit_complexity}, we discuss the bit complexity of SOS proofs, and show how $k$-effectiveness can be used to prove the existence of SOS proofs with small bit complexity. In \prettyref{cha:symmetric_sdps} we discuss the optimality of the SOS relaxations, and show how this implies an exponential size lower bound for approximating \textsc{Matching}. Finally, in \prettyref{cha:future_work} we discuss a few open problems and open threads continuing the lines of research of this thesis.