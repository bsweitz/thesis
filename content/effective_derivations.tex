\chapter{Effective Derivations}\label{cha:effective_derivations}
In this section we prove that many natural sets of polynomials $\cP$ arising from polynomial formulations for combinatorial optimization problems are effective. This means that the problem of determining if a polynomial $p$ is in the ideal $\gen{\cP}$ has a simple solution in time polynomial in $n^{\deg p}$. When $\cP$ is complete, this is equivalent to determining if $p(\alpha) = 0$ for every $\alpha \in V(\cP)$. In \prettyref{cha:bit_complexity} and \prettyref{cha:symmetric_sdps} we will see two applications of solving this problem. We start with the sets of polynomials that are "most complete" for their ideals.

\section{Gr\"obner Bases}
Recall the definition of Gr\"obner basis from \prettyref{def:grobner}.
\begin{lemma}\label{lem:grobnereffective}
Let $\cP$ be a Gr\"obner basis. Then $\cP$ is $1$-effective in the PC proof system.
\end{lemma}
\begin{proof}
By \prettyref{prop:grobner-unique}, total reduction by $\cP$ is well-defined, and in fact there is a unique 
remainder. Let $r \in \gen{\cP}$ be of degree $d$, and consider the total reduction of $r$ by $\cP$. Because $r \in \gen{\cP}$, the only total reduction of $r$ by $\cP$ is $0$. If we enumerate the polynomials that are produced by the iterative reductions $r = r_0, r_1, \dots, r_N = 0$, then 
$r_i = r_{i+1} + q_{i+1}p_{i+1}$, where $p_{i+1} \in \cP$, $\deg r_{i+1} \leq \deg r_i$, and $\deg q_{i+1}p_{i+1} \leq \deg r_i$. Combining all these sums into one, we get
$r = \sum_i q_i p_i$, which is a derivation of degree $d$. 
\end{proof}
\prettyref{lem:grobnereffective} is trivial to prove, but there are several important combinatorial optimization problems that fall 
under its umbrella, like \prettyref{ex:grobner}:
\begin{corollary}\label{cor:csp-effective}
\textsc{CSP} is $1$-effective.
\end{corollary}
\begin{proof}
Recall that \textsc{CSP} corresponds to the set of constraints $\cP = \{x_i^2 - x_i | i \in [n]\}$. We prove this is a Gr\"obner basis. 
Let $p \in \gen{\cP}$. If $p$ is not multilinear, we can divide $p$ by elements of $\cP$ until we have a multilinear remainder $r$. Because 
$p \in \gen{\cP}$ and each element of $\cP$ is zero on the hypercube $\{0, 1\}^n$, $r$ must also be zero on the hypercube. But the multilinear
polynomials form a basis for functions on the hypercube, so if $r$ is a multilinear polynomial which is zero, then it must be the zero polynomial. 
\end{proof}
\begin{corollary}
\textsc{Clique} is $1$-effective.
\end{corollary}
\begin{proof}
Recall that \textsc{Clique}$(V,E)$ corresponds to the set of constraints $\cP = \{x_i^2 - x_i | i \in [|V|]\} \cup \{ x_ix_j | (i,j) \notin E\}$. 
We prove this is a Gr\"obner basis. Let $p \in \gen{\cP}$. If $p$ is not multilinear, we can divide it until we have a multilinear
remainder $r_1$. Now by dividing $r_1$ by the polynomials in the second part of $\cP$, we can remove all monomials containing $x_ix_j$ where $(i,j) \notin E$
to get $r_2$. Thus $r_2$ contains only monomials which are cliques of varying sizes in the graph $(V,E)$. Let $C$ be the smallest clique with a
nonzero coefficient $\alpha_C$ in $r_2$. Let $\chi_C$ be the characteristic vector of $C$, i.e. $(\chi_C)_i = 1$ if $i \in C$, and $(\chi_C)_i = 0$ otherwise.
Then $r_2(\chi_C) = \alpha_C$. But $p(\chi_C) = 0$ for every $p \in \cP$, and $r_2 \in \gen{\cP}$. Thus $\alpha_C = 0$, a contradiction, and so $r_2$ is the 
zero polynomial.
\end{proof}

However, not every problem falls so neatly into this classification. There are many natural problems whose solution spaces have a small set of generating
polynomials which are not Gr\"obner bases, and indeed their Gr\"obner bases can be exponentially large and exceedingly difficult to compute. Despite this,
these problems can still admit effective derivations, as we prove here.

\section{Proof Strategy for Symmetric Solution Spaces}
In this section we describe our main proof strategy to show that a set of polynomials $\cP$ is effective. 
We apply this strategy to combinatorial optimization problems which have a natural symmetry to their solution spaces $V(\cP)$. 
For each of these problems, we will define an $S_m$-action on $[n]$, which extends to an action on $\R[x_1,\dots,x_n]$ as well as $\R^n$ by permutation of variable names and indices respectively. 
The action will be a natural permutation of the solutions. For example, for \textsc{Matching}, the group action will correspond to simply permuting the vertices of the complete graph. 

With the group action in hand, our proof strategy follows in three steps:
\begin{enumerate}
\item[(1)] Prove that $\cP$ is complete. This is usually done by exhibiting a degree $n$ derivation from $\cP$ for any polynomial $p$ which is zero on $V(\cP)$. This step is essential for the induction in step (3). 
\item[(2)] Prove that for every $p \in \gen{\cP}$, the polynomial $\frac{1}{n!} \sum_{\sigma \in S_m} \sigma p$ has a derivation from $\cP$ in degree $\deg p$. This is usually fairly easy from elements of $\cP$ that look like $\sum_i x_i$. 
\item[(3)] Prove that for every $\sigma \in S_m$, $p - \sigma p$ has a derivation from $\cP$ in degree at most $k \deg p$, for some constant $k$. This is performed by induction on a natural parameter of the combinatorial optimization problem. The more complicated the solution space for the combinatorial optimization problem, the worse the constant $k$ gets. 
\end{enumerate}
We use this general strategy to prove that many polynomial formulations for different natural combinatorial optimization problems admit effective derivations. 
Our efforts to find a unifying theory that explains the effectiveness of this strategy on the different problems have failed, so we have to prove that each $\cP$ is effective on a case-by-case basis.  

%Intuitively speaking, this means that their solution spaces are simple and easily explained. Any polynomial fact over the solution space
%can be proven using a proof of only roughly the same complexity of the fact.  
\section{Effective Derivations for \textsc{Matching}}
Fix an even integer $n$, and recall that the matching constraints are polynomials in $\R^{\binom{n}{2}}$ (we abuse notation and use $x_{ij}$ and $x_{ji}$ equivalently):
\begin{align}
\pmatch(n) = \matching.\label{eq:matching-formulation}
\end{align}
We omit the dependence on $n$ when it is clear from context. For an element $\sigma$ of the symmetric group $\cS_n$, we define the action of $\sigma$ on a variable by $\sigma x_{ij} = x_{\sigma(i)\sigma(j)}$.
We define the action of $\sigma$ on a monomial by extending this action multiplicatively, and the action of $\sigma$ on a full polynomial by extending linearly.
Note that $\pmatch$ is fixed by the action of every $\sigma$, as are its solutions $V(\pmatch)$ corresponding to the matchings of $K_n$. 
Thus for any $p \in \pmatch$, we also have $\sigma p \in \pmatch$. For a partial matching $M$, i.e. a set of disjoint pairs from $[n]$, define $x_M = \prod_{e \in M} x_e$ with the convention that $x_{\emptyset} = 1$.
First, we note an easy lemma on the structure of polynomials in $\gen{\pmatch}$:
\begin{lemma}\label{lem:monomials}
  Let $p$ be any polynomial. Then there is a multilinear polynomial $q$ such that every monomial of $q$ is a partial matching monomial, and $p-q$ has a derivation from $\pmatch$ of degree $\deg p$.
\end{lemma}
\begin{proof}
It suffices to prove the lemma when \(p\) is a monomial. Let
\(p = \prod_{e \in A} x_{e}^{k_{e}}\)
for a set \(A\) of edges with multiplicities \(k_{e} \geq 1\).
From the constraint \(x_{e}^{2} - x_e\), it follows that
$\prod_{e \in A} x_e^{k_e} - \prod_{e \in A} x_e$ has a derivation from $\pmatch$ in degree $\deg p$.
Now if $A$ is a partial matching we are done, otherwise there exist edges $f,g \in A$ which are not disjoint.
But then $x_fx_g \in \pmatch$, and so $\prod_{e \in A} x_e$ has a derivation from $\pmatch$ in degree $|A|$, which implies the statement.
\end{proof}
With \prettyref{lem:monomials} in hand, we prove the following easy result:
\begin{lemma}\label{lem:matching-complete}
Then $\gen{\pmatch(n)}$ is complete for any even $n$. 
\end{lemma}
\begin{proof}
Let $p$ be a polynomial such that $p(\alpha) = 0$ for each $\alpha \in V(\pmatch)$. By \prettyref{lem:monomials}, we can assume that $p(x)$ is a multilinear polynomial whose monomials correspond to partial matchings. For such a partial matching $M$, clearly $x_M - x_M\prod_{u \notin M} \sum_v x_{uv}$ has a derivation in degree $n$ using the constraints $\sum_v x_{uv} - 1 \in \pmatch$. By eliminating terms which do not correspond to partial matchings, we get $x_M - \sum_{M': M \subset M'} x_{M'} \in \gen{\pmatch}$. Doing this to every monomial, we determine there is a polynomial $p'$ which is homogeneous of degree $n$ such that $p - p' \in \gen{\pmatch}$. Now since the coefficients of $p'$ correspond exactly to perfect matchings, for each $\alpha \in V(\pmatch)$ there is a monomial in $p'$ whose coefficient is exactly $p'(\alpha)$. Since $p'(\alpha) = 0$ for every $\alpha \in V(\pmatch)$, it must be that $p' = 0$, and so $p \in \gen{\pmatch}$.
\end{proof}
Now we move on to the second step of our proof. 

\subsection{Symmetric Polynomials}

We will prove the following lemma: 
\begin{lemma}
  \label{lem:constant}
  Let $p$ be a polynomial in $\R^{\binom{n}{2}}$.
	Then there is a constant $c_p$ such that $\sum_{\sigma \in \cS_n} \sigma p - c_p$ has a derivation from $\pmatch$ in degree at most $\deg p$. 
\end{lemma}
To do so, it will be useful to first prove a few lemmas on how we can simplify the structure of $p$. 

Any partial matching monomial may be extended as a sum over partial matching monomials containing that partial matching using the constraint $\sum_j x_{ij} - 1 \in \pmatch$. The first lemma here shows how to extend by a single edge, and the second iteratively applies this process to extend by multiple edges.
\begin{lemma}
  \label{lem:matching+a}
  For any partial matching \(M\) on \(2d\) vertices
  and a vertex \(u\) not covered by \(M\),
  \begin{equation}
    \label{eq:matching+a}
    x_{M}
    \cong
    \sum_{\substack{M_{1} = M \cup \{u,v\}: \\
        v \in [n] \setminus (M \cup \{u\})}}
    x_{M_{1}}
    ,
  \end{equation}
	and the derivation can be done in degree $d+1$.
\end{lemma}
\begin{proof}
We use the constraints \(\sum_{v} x_{uv} - 1\)
to add variables corresponding to edges at \(u\),
and then use \(x_{uv} x_{uw}\) to remove monomials
not corresponding to a partial matching:
\begin{equation*}
  x_{M}
  \cong
  x_{M} \sum_{v \in K_n} x_{uv}
  \cong
  \sum_{\substack{M_{1} = M \cup \{u,v\}: \\
      v \in K_{n} \setminus (M \cup \{u\})}}
  x_{M_{1}}
  .
\end{equation*}
It is easy to see that these derivations are done in degree $d+1$.
\end{proof}
\begin{lemma}
  \label{lem:partial-matching}
  For any partial matching \(M\) of \(2d\) vertices
  and \(d \leq k \leq n/2\),
  we have
  \begin{equation}
    \label{eq:partial-matching}
    x_{M} \cong
    \frac{1}{\binom{n/2 - d}{k - d}}
    \sum_{\substack{M' \supset M \\ \size{M'} = k}} x_{M'}
  \end{equation}
\end{lemma}
\begin{proof}
We use induction on \(k - d\).
The start of the induction is with \(k = d\),
when the sides of \prettyref{eq:partial-matching}
are actually equal. If \(k > d\), let \(u\) be a fixed vertex not covered by \(M\).
Applying \prettyref{lem:matching+a} to \(M\) and \(u\)
followed by the inductive hypothesis gives:
\begin{equation*}
	\begin{split}
    x_{M}
    &\cong
    \sum_{\substack{M_{1} = M \cup \{u,v\}: \\
        v \in K_{n} \setminus (M \cup \{u\})}}
    x_{M_{1}} \\
    &\cong
    \frac{1}{\binom{n/2 - d - 1}{k - d - 1}}
    \sum_{\substack{
        M' \supset M_{1} \\ \size{M'} = k \\
        M_{1} = M \cup \{u,v\}: \\
        v \in K_{n} \setminus (M \cup \{u\})}}
    x_{M'}
    \end{split}
    .
\end{equation*}
Averaging over all vertices \(u\) not covered by \(M\),
we obtain:
\begin{equation*}
  \begin{split}
  x_{M}
  &\cong
  \frac{1}{n - 2 d}
  \frac{1}{\binom{n/2 - d - 1}{k - d - 1}}
  \sum_{\substack{
      M' \supset M_{1} \\ \size{M'} = k \\
      M_{1} = M \cup \{u,v\}: \\
      \{u, v\} \in K_{n} \setminus M}}
  x_{M'} \\
  &=
  \frac{1}{n - 2 d}
  \frac{1}{\binom{n/2 - d - 1}{k - d - 1}}
  2 (k - d)
  \sum_{\substack{
      M' \supset M \\ \size{M'} = k}}
  x_{M'} \\
  &=
  \frac{1}{\binom{n/2 - d}{k - d}}
  \sum_{\substack{M' \supset M \\ \size{M'} = k}}
  x_{M'}
  \end{split}
  .
\end{equation*}
where in the second step the factor \(2(k - d)\) accounts for the different choices of $\{u,v\}$ that can lead to extending $M$ to $M'$.
\end{proof}
For a partial matching \(M\),
let \(x_{M} \coloneqq \prod_{e \in M} x_{e}\)
denote the product of edge variables for the edges in \(M\).

Finally, we can prove the first main lemma:
\begin{proof}[Proof of \prettyref{lem:constant}]
Given \prettyref{lem:monomials},
it suffices to prove the claim for
\(p = x_{M}\) for some partial matching \(M\).
Let $\deg p = |M| = k$.
Note that $\cS_n$ acts transitively on the monomials of degree $k$, and thus $2^k k! (n-2k)!$ elements of $\cS_n$ stabilize $p$.
Thus $\sum_{\sigma \in S_n} \sigma x_M = 2^k k! (n-2k)!\sum_{M': |M'| = k} x_{M'}$.
Finally, apply \prettyref{lem:partial-matching} with $d = 0$:
\begin{equation*}
  \begin{split}
  \sum_{\sigma \in \cS_{n}} \sigma x_{M}
  &= 2^{k} k! (n-2k)! \sum_{M' \colon \size{M'} = k} x_{M'} \\
  &\cong
  2^{k} k! (n-2k)! \binom{n/2}{k}.
  \end{split}
\end{equation*}
\end{proof}
As a corollary, we get that when $p \in \gen{\pmatch}$, then the constant must be zero 
\begin{corollary}\label{cor:constantiszero}
If $p \in \gen{\pmatch}$, then $\sum_{\sigma \in \cS_n} \sigma p$ has a derivation from $\pmatch$ in degree $\deg p$.
\end{corollary}
\begin{proof}
Apply \prettyref{lem:constant} to obtain a constant $c_p$ such that $\sum_{\sigma \in \cS_n} \sigma p \cong c_p$. 
Now since $p \in \gen{\pmatch}$, $c_p \in \gen{\pmatch}$ as well. But the only constant polynomial in $\gen{\pmatch}$ is $0$.
\end{proof}

\subsection{Getting to a Symmetric Polynomial}
In order to apply \prettyref{lem:constant} to a general polynomial $p$, we need to show how to derive the difference polynomial $p - \sum_{\sigma \in \cS_n} \sigma p$ from $\pmatch$. Our proof will be by an induction on the number of vertices $n$. Because the number of vertices will be changing in this section, we will stop omitting the dependence on $n$.
The next lemma will allow us to apply induction:
\begin{lemma}
  \label{lem:degree-increase}
  Let \(L\) be a polynomial with a degree $d$ derivation from $\pmatch(n)$.
	Then $Lx_{n+1,n+2}$ has a degree $d+1$ derivation from $\pmatch(n+2)$.
\end{lemma}
\begin{proof}
It suffices to prove the statement for $L \in \pmatch(n)$. 
If $L = x_{ij}^2 - x_{ij}$ or $L = x_{ij}x_{ik}$, the claim clearly true because $L \in \pmatch(n+2)$.
Then let $L = \sum_j x_{ij} - 1$, note that 
\begin{align*}
Lx_{n+1}x_{n+2} &= (\sum_{j=1}^n x_{ij} - 1)x_{n+1,n+2} \\
&= (\sum_{j=1}^{n+2} x_{ij} - 1)x_{n+1,n+2} + (-1)\cdot x_{i,n+1}x_{n+1,n+2} + (-1)\cdot x_{i,n+2}x_{n+1,n+2},
\end{align*}
and realize that all three terms contain a factor in $\pmatch(n+2)$.
\end{proof}

We are now ready to prove the main theorem of this section, that the matching constraints $\pmatch(n)$ admit effective derivations. 
\begin{theorem}\label{thm:matching-effective}
Let $p \in \gen{\pmatch(n)}$, and let $d = \deg p$. Then $p$ has a derivation from $\pmatch(n)$ in degree $2d$. 
\end{theorem}
\begin{proof}
By \prettyref{lem:monomials}, we can assume that $p$ is a multilinear polynomial whose monomials correspond to partial matchings.
As promised, our proof is by induction on $n$. Consider the base case of $n = 2$. 
Then $V(\pmatch(2)) = \{1\}$ and either $p$ is a constant or linear polynomial.
The only such polynomials that are zero on $V(\pmatch(2))$ are $0$ and scalar multiples of $x_{12} - 1$. 
The former case has the trivial derivation, and the latter case is simply an element of $\pmatch(2)$. 

Now assume that for any $d$, the theorem statement holds for polynomials in $\gen{\pmatch(n')}$ for any $n' < n$. 
Let $p \in \gen{\pmatch(n)}$ be multilinear of degree $d$ whose monomials correspond to partial matchings, and let $\sigma = (i,j)$ be a transposition of two vertices.
We consider the polynomial $\Delta = p - \sigma p$. Note that $\Delta \in \gen{\pmatch(n)}$, and any monomial which does not match either $i$ or $j$, or a monomial which matches $i$ to $j$, will not appear in $\Delta$ as it will be canceled by the subtraction.
Thus we can write
\[\Delta = \sum_{e: i \in e \text{ or } j \in e} L_e x_e,\]
with each $L_e$ having degree at most $d-1$. As foreshadowed earlier, our goal is to remove two of the variables in these matchings in order to apply induction. In order to do that, we will need each term to depend not only on either $i$ or $j$, but both. To this end, we multiply each term by the appropriate polynomial $\sum_k x_{ik}$ or $\sum_k x_{jk}$ to obtain
\[\Delta \cong \sum_{k_1k_2} L_{k_1k_2} x_{ik_1}x_{jk_2}.\]
We can think of the RHS polynomial as being a partition over the possible different ways to match $i$ and $j$.
Furthermore, because of the constraints of type $x_{ij}x_{ik}$, we can take $L_{k_1k_2}$ to be independent of $x_e$ for any $e$ incident to any of $i,j,k_1,k_2$. 
We argue that $L_{k_1k_2} \in \pmatch(n-4)$. We know that $\Delta(\alpha) = 0$ for any $\alpha \in V(\pmatch(n))$. Let $\alpha \in V(\pmatch(n))$ such that $\alpha_{ik_1} = 1$ and $\alpha_{jk_2} = 1$. 
Then it must be that $\alpha_{ik} = 0$ and $\alpha_{jk} = 0$ for any other $k$, since otherwise $\alpha \notin V(\pmatch(n))$.
Thus $\Delta(\alpha) = L_{k_1k_2}(\alpha)$. Since $L_{k_1k_2}$ is independent of any edge incident to $i,j,k_1,k_2$, it does not involve those variables, so $L_{k_1k_2}(\alpha) = L_{k_1k_2}(\beta)$, where $\beta$ is the restriction of $\alpha$ to the $\binom{n-4}{2}$ variables which $L_{k_1k_2}$ depends on. But such a $\beta$ is simply an element of $V(\pmatch(n-4))$, and all elements of $V(\pmatch(n-4))$ can be obtained this way. Thus $L_{k_1k_2}$ is zero on all of $V(\pmatch(n-4))$, and by \prettyref{lem:matching-complete}, $L_{k_1k_2} \in \gen{\pmatch(n-4)}$. Now by the inductive hypothesis, $L_{k_1k_2}$ has a derivation from $\pmatch(n-4)$ of degree at most $2d-2$. By two applications of \prettyref{lem:degree-increase}, $L_{k_1k_2}x_{ik_1}x_{jk_2}$ has a derivation from $\pmatch(n)$ of degree at most $2d$, and thus so does $\Delta$.

Because transpositions generate the symmetric group, the above argument implies that $p - \frac{1}{n!}\sum_{\sigma \in \cS_n} \sigma p$ has a derivation from $\pmatch(n)$ of degree at most $2d$. Combined with \prettyref{lem:constantiszero}, this is enough to prove the theorem statement. 
\end{proof}

\section{Effective Derivations for \textsc{TSP}}

For each integer $n$, a polynomial formulation with $n^2$ variables for \textsc{TSP} on $n$ vertices uses the following polynomials:
\begin{align*}
\ptsp(n) &= \tsp,\label{eq:tsp-formulation}
\end{align*}
where a tour $\tau \in S_n$ (which is a feasible solution for \textsc{TSP}) is identified with the vector $\chi_\tau(i,j) = 1$ if $\tau(i) = j$ and $0$ otherwise. We omit the dependence on $n$ if it is clear from context. For an element $\sigma$ of the symmetric group $S_n$, we define the action of $\sigma$ on a variable by $\sigma x_{ij} = x_{\sigma(i)j}$.
We define the action of $\sigma$ on a monomial by extending this action multiplicatively, and the action of $\sigma$ on a full polynomial by extending linearly.
Then $\ptsp$ is fixed by the action of every $\sigma$, as are its solutions $V(\ptsp)$ corresponding to the tours. 

Note that $V(\ptsp)$ corresponds to a matching on $K_{n,n}$, the complete bipartite graph on $2n$ vertices. Thus it should come as no surprise that the same proof strategy as the one we used for matchings on the complete graph $K_n$ should go through. This section will be extremely similar to the previous one, and the reader loses very little by skipping ahead to \prettyref{sec:bcsp}. It would be much nicer if we just reduce to $\ptsp(n)$ from $\pmatch(2n)$. This requires proving that any polynomial which is zero on $V(\ptsp(n))$ is the projection of a polynomial of similar degree which is zero on $V(\pmatch(2n))$. Unfortunately I am not sure how to prove this except by proving that $\ptsp$ is effective, so we will have to live with some repetition. 

For a partial matching $M$ of $K_{n,n}$, i.e. a set of disjoint pairs from $[n] \times [n]$, define $x_M = \prod_{e \in M} x_e$ with the convention that $x_{\emptyset} = 1$. We also define $M_L = \{i \in [n] | \exists j: (i,j) \in M\}$ and $M_R = \{j \in [n] | \exists i: (i,j) \in M\}$. 
\begin{lemma}\label{lem:tsp-monomials}
 Let $p$ be any polynomial. Then there is a multilinear polynomial $q$ such that every monomial of $q$ is a partial matching monomial, and $p-q$ has a derivation from $\cP$ of degree $\deg p$.
\end{lemma}
\begin{proof}
The statement follows easily by using the elements of $\ptsp$ of the form $x_{ij}^2 - x_{ij}$ to make a multilinear polynomial, then eliminating any monomial which is not a partial matching by using elements of the form $x_{ij}x_{ik}$ or $x_{ji}x_{ki}$. 
\end{proof}
With \prettyref{lem:tsp-monomials} in hand, we prove the following easy result:
\begin{lemma}\label{lem:tsp-complete}
$\gen{\ptsp(n)}$ is complete for any $n$.  
\end{lemma}
\begin{proof}
Let $p$ be a polynomial such that $p(\alpha) = 0$ for each $\alpha \in V(\ptsp)$. By \prettyref{lem:tsp-monomials}, we can assume that $p(x)$ is a multilinear polynomial whose monomials correspond to partial matchings. For such a partial matching $M$, clearly $x_M - x_M\prod_{i \notin M} \sum_j x_{ij}$ has a derivation in degree $n$ using the constraints $\sum_j x_{ij} - 1 \in \ptsp$. By eliminating terms which do not correspond to partial matchings, we get $x_M - \sum_{M': M \subset M'} x_{M'} \in \gen{\cP}$. Doing this to every monomial, we determine there is a polynomial $p'$ which is homogeneous of degree $n$ such that $p - p' \in \gen{\cP}$. Now since the monomials of $p'$ correspond to perfect matchings, each monomial has an $\alpha$ such that the coefficient of that monomial is exactly $p'(\alpha)$. Since $p'(\alpha) = 0$ for every $\alpha \in V(\ptsp)$, it must be that $p' = 0$, and so $p \in \gen{\ptsp}$.
\end{proof}
Now we move on to the second step of our proof. 

\subsection{Symmetric Polynomials}

We will complete this step of our proof using the same helper lemmas as for \textsc{Matching}. The numbers appearing are slightly different due to the difference in the number of partial matchings for $K_n$ and $K_{n,n}$, and the action of $\sym{n}$ is slightly different, but they are all basically the same lemmas.

\begin{lemma}
  \label{lem:tsp+a}
  For any partial matching \(M\) on \(2d\) vertices
  and a vertex \(i \in [n] \setminus M_L\),
  \begin{equation}
    \label{eq:tsp+a}
    x_{M}
    \cong
    \sum_{\substack{M_{1} = M \cup \{i,j\}: \\
        j \in [n] \setminus (M_R)}}
    x_{M_{1}}
    ,
  \end{equation}
	and the derivation can be done in degree $d+1$.
\end{lemma}
\begin{proof}
We use the constraints \(\sum_{v} x_{uv} - 1\)
to add variables corresponding to edges at \(u\),
and then use \(x_{uv} x_{uw}\) to remove monomials
not corresponding to a partial matching:
\begin{equation*}
  x_{M}
  \cong
  x_{M} \sum_{j \in [n]} x_{ij}
  \cong
  \sum_{\substack{M_{1} = M \cup \{i,j\}: \\
      j \in [n] \setminus M_R}}
  x_{M_{1}}
  .
\end{equation*}
It is easy to see that these derivations are done in degree $d+1$.
\end{proof}
\begin{lemma}
  \label{lem:tsp-partial-matching}
  For any partial matching \(M\) of \(2d\) vertices
  and \(d \leq k \leq n\),
  we have
  \begin{equation}
    \label{eq:tsp-partial-matching}
    x_{M} \cong
    \frac{1}{\binom{n - d}{k - d}}
    \sum_{\substack{M' \supset M \\ \size{M'} = k}} x_{M'}
  \end{equation}
\end{lemma}
\begin{proof}
We use induction on \(k - d\).
The start of the induction is with \(k = d\),
when the sides of \prettyref{eq:tsp-partial-matching}
are actually equal. If \(k > d\), let \(i\) be a fixed vertex not in \(M_L\).
Applying \prettyref{lem:tsp-matching+a} to \(M\) and \(i\)
followed by the inductive hypothesis gives:
\begin{equation*}
	\begin{split}
    x_{M}
    &\cong
    \sum_{\substack{M_{1} = M \cup \{i,j\}: \\
        j \in [n] \setminus M_R}}
    x_{M_{1}} \\
    &\cong
    \frac{1}{\binom{n - d - 1}{k - d - 1}}
    \sum_{\substack{
        M' \supset M_{1} \\ \size{M'} = k \\
        M_{1} = M \cup \{i,j\}: \\
        j \in [n] \setminus M_R}}
    x_{M'}
    \end{split}
    .
\end{equation*}
Averaging over all vertices \(i\) not in $M_L$,
we obtain:
\begin{equation*}
  \begin{split}
  x_{M}
  &\cong
  \frac{1}{n - d}
  \frac{1}{\binom{n - d - 1}{k - d - 1}}
  \sum_{\substack{
      M' \supset M_{1} \\ \size{M'} = k \\
      M_{1} = M \cup \{i,j\}: \\
      \{i,j\} \in [n]\times[n] \setminus M}}
  x_{M'} \\
  &=
  \frac{1}{n - d}
  \frac{1}{\binom{n - d - 1}{k - d - 1}}
  (k - d)
  \sum_{\substack{
      M' \supset M \\ \size{M'} = k}}
  x_{M'} \\
  &=
  \frac{1}{\binom{n - d}{k - d}}
  \sum_{\substack{M' \supset M \\ \size{M'} = k}}
  x_{M'}
  \end{split}
  .
\end{equation*}
where in the second step the factor \((k - d)\) accounts for the different choices of $\{i,j\}$ that can lead to extending $M$ to $M'$.
\end{proof}
\begin{lemma}
  \label{lem:tsp-constant}
  Let $p$ be a polynomial in $\R^{n^2}$.
	Then there is a constant $c_p$ such that $\sum_{\sigma \in \sym{n}} \sigma p - c_p$ has a derivation from $\ptsp$ in degree at most $\deg p$. 
\end{lemma}
\begin{proof}
Given \prettyref{lem:tsp-monomials},
it suffices to prove the claim for
\(p = x_{M}\) for some partial matching \(M\).
Let $\deg p = |M| = k$.
There are $(n-k)!$ elements of $\sym{n}$ that stabilize a given partial matching $M$, so
$\sum_{\sigma \in \sym{n}} \sigma x_M = (n-k)!\sum_{M': |M'| = k} x_{M'}$.
Finally, apply \prettyref{lem:tsp-partial-matching} with $d = 0$:
\begin{equation*}
  \begin{split}
  \sum_{\sigma \in \sym{n}} \sigma x_{M}
  &= (n-k)! \sum_{M' \colon \size{M'} = k} x_{M'} \\
  &\cong
  (n-k)!\binom{n}{k}.
  \end{split}
\end{equation*}
\end{proof}
As a corollary, we get that when $p \in \gen{\cP}$, then the constant must be zero 
\begin{corollary}\label{cor:tsp-constantiszero}
If $p \in \gen{\ptsp}$, then $\sum_{\sigma \in \sym{n}} \sigma p$ has a derivation from $\ptsp$ in degree $\deg p$.
\end{corollary}
\begin{proof}
Apply \prettyref{lem:tsp-constant} to obtain a constant $c_p$ such that $\sum_{\sigma \in \sym{n}} \sigma p \cong c_p$. 
Now since $p \in \gen{\ptsp}$, $c_p \in \gen{\ptsp}$ as well. But the only constant polynomial in $\gen{\ptsp}$ is $0$.
\end{proof}

\subsection{Getting to a Symmetric Polynomial}
The third step also proceeds in an almost identical manner.
\begin{lemma}
  \label{lem:tsp-degree-increase}
  Let \(L\) be a polynomial with a degree $d$ derivation from $\ptsp(n)$.
	Then $Lx_{n+1,n+2}x_{n+2,n+1}$ has a degree $d+2$ derivation from $\ptsp(n+2)$.
\end{lemma}
\begin{proof}
It suffices to prove the statement for $L \in \ptsp(n)$. 
If $L = x_{ij}^2 - x_{ij}$, $L = x_{ij}x_{ik}$, or $L = x_{ji}x_{ki}$, the claim is clearly true because $L \in \ptsp(n+2)$.
Then let $L = \sum_j x_{ij} - 1$ for some $i$, and note that 
\begin{align*}
Lx_{n+1,n+2}x_{n+2,n+1} &- \left(\sum_{j=1}^{n+2} x_{ij} - \right)x_{n+1,n+2}x_{n+2,n+1} = \left(x_{i,n+1} + x_{i,n+2}\right)x_{n+1,n+2}x_{n+2,n+1} \\
&= \left(x_{i,n+1}x_{n+2,n+1}\right)x_{n+1,n+2} + \left(x_{i,n+2}x_{n+1,n+2}\right)x_{n+2,n+1} \\
&\cong 0
\end{align*}
The case for $L = \sum_i x_{ij} - 1$ is symmetric.
\end{proof}

We are now ready to prove the main theorem of this section, that the \textsc{TSP} constraints $\ptsp$ admit effective derivations. 
\begin{theorem}\label{thm:tsp-effective}
Let $p \in \gen{\ptsp(n)}$ for any $n$, and let $d = \deg p$. Then $p$ has a derivation from $\ptsp(n)$ in degree $2d$. 
\end{theorem}
\begin{proof}
By \prettyref{lem:tsp-monomials}, we can assume that $p$ is a multilinear polynomial whose monomials correspond to partial matchings on $K_{n,n}$.
As before, our proof is by induction on $n$. Consider the base case of $n = 1$. 
Then $V(\ptsp(1)) = \{1\}$ and either $p$ is a constant or linear polynomial (since there is only one varaible, $x_{11}$).
The only such polynomials that are zero on $V(\ptsp(1))$ are $0$ and scalar multiples of $x_{11} - 1$. 
The former case has the trivial derivation, and the latter case is simply an element of $\ptsp(1)$. 

Now assume that for any $d$, the theorem statement holds for polynomials in $\gen{\ptsp(n')}$ for any $n' < n$. 
Let $p \in \gen{\ptsp(n)}$ be multilinear of degree $d$ whose monomials correspond to partial matchings, and let $\sigma = (i,j)$ be a transposition of two left indices.
We consider the polynomial $\Delta = p - \sigma p$. Note that $\Delta \in \gen{\ptsp(n)}$, and any monomial which does not match either $i$ or $j$ will not appear in $\Delta$ as it will be canceled by the subtraction.
Thus we can write
\[\Delta = \sum_{e: e = (i,k) \text{ or } e = (j,k)} L_e x_e,\]
with each $L_e$ having degree at most $d-1$. Proceeding as before, we multiply each term by the appropriate constraint $\sum_k x_{ik}$ or $\sum_j x_{jk}$ to obtain a decomposition
\[\Delta \cong \sum_{k_1,k_2} L_{k_1k_2} x_{ik_1}x_{jk_2}.\]
We can think of the RHS polynomial as being a partition over the possible different ways to match $i$ and $j$.
Furthermore we can take $L_{k_1k_2}$ to be independent of $x_e$ for any $e$ incident to any of $i,j,k_1,k_2$. 
We argue that $L_{k_1k_2} \in \ptsp(n-2)$. We know that $\Delta(\alpha) = 0$ for any $\alpha \in V(\ptsp(n))$. Let $\alpha \in V(\ptsp(n))$ such that $\alpha_{ik_1} = 1$ and $\alpha_{jk_2} = 1$. 
Then it must be that $\alpha_{ik} = 0$ and $\alpha_{jk} = 0$ for any other $k$, since otherwise $\alpha \notin V(\ptsp(n))$.
Thus $\Delta(\alpha) = L_{k_1k_2}(\alpha)$. Since $L_{k_1k_2}$ is independent of any edge incident to $i,j,k_1,k_2$, it does not involve those variables, so $L_{k_1k_2}(\alpha) = L_{k_1k_2}(\beta)$, where $\beta$ is the restriction of $\alpha$ to the $(n-2)^2$ variables which $L_{k_1k_2}$ depends on. But such a $\beta$ is simply an element of $V(\ptsp(n-2))$, and all elements of $V(\ptsp(n-2))$ can be obtained this way. Thus $L_{k_1k_2}$ is zero on all of $V(\ptsp(n-2))$, and by \prettyref{lem:tsp-complete}, $L_{k_1k_2} \in \gen{\ptsp(n-2)}$. Now by the inductive hypothesis, $L_{k_1k_2}$ has a derivation from $\ptsp(n-2)$ of degree at most $2d-2$. By \prettyref{lem:tsp-degree-increase}, $L_{k_1k_2}x_{ik_1}x_{jk_2}$ has a derivation from $\ptsp(n)$ of degree at most $2d$, and thus so does $\Delta$.

Because transpositions generate the symmetric group, the above argument implies that $p - \frac{1}{n!}\sum_{\sigma \in \sym{n}} \sigma p$ has a derivation from $\ptsp(n)$ of degree at most $2d$. Combined with \prettyref{cor:tsp-constantiszero}, this is enough to prove the theorem statement. 
\end{proof}

\section{Effective Derivations for \textsc{Balanced-CSP}}\label{sec:bcsp}
Fix an integer $n$, and recall that the \textsc{Balanced-CSP} constraints are polynomials in $\R^n$:
\begin{align}
\pbcsp(n,c) = \left\{x_i^2 - x_i| i \in [n]\right\} \cup \left\{\sum_i x_i - c\right\}.\label{eq:bcsp-formulation}
\end{align}
The \textsc{Bisection} constraints are the special case when $n$ is even and $c = n/2$. 
As before, we need to define the appropriate symmetric action. For an element $\sigma \in \cS_n$, we define $\sigma x_i = x_{\sigma(i)}$ and extend this action multiplicatively and linearly to get an action on every polynomial. 
Once again, note that $\pbcsp$ and $V(\pbcsp)$ are fixed by $\cS_n$ under this action, and thus if $p \in \gen{\pbcsp}$, then $\sigma p \in \gen{\pbcsp}$. 
We will begin with the special case of \textsc{Bisection}, as we will encounter an obstacle for general $c$. 
Because $\pbcsp$ contains the boolean constraints $\{x_i^2 - x_i | i \in [n]\}$, we will take $p$ to be a multilinear polynomial. 
Our proof strategy is the same two-step strategy as before. 
\begin{lemma}\label{lem:bcsp-complete}
$\gen{\pbcsp(n,c)}$ is complete for any $n$ and $c \leq n$.
\end{lemma}
\begin{proof}
Let $p$ be a polynomial which is zero on all of $V(\pbcsp)$. First, because of the boolean constraints in $\pbcsp$ we can assume that $p$ is multilinear. First, we argue that if $A$ is such that $|A| > c$ then $x_A \in \gen{\pbcsp}$. We prove this by backwards induction from $n$ to $c+1$. For the base case of $|A| = n$, note that 
\[x_A \cong \frac{1}{n-c} x_A(\sum_i x_i - c).\]
Now if $|A| = k$ with $c+1 \leq k < n$, we have
\[\left(k-c\right)x_A + \sum_{i \notin A} x_{A \cup \{i\}} \cong x_A(\sum_i x_i - c).\]
By the inductive hypothesis, the second term is in $\gen{\pbcsp}$, and thus so is $x_A$. Thus we can assume that the monomials of $p$ are all of degree at most $c$. For any monomial $x_A$ of $p$, we have $x_A(\sum_i x_i - 1) \cong \sum_{i \notin A} x_{A \cup \{i\}} - (c-|A|)x_A$, and so $x_A - \frac{1}{c - |A|}\sum_{i \notin A} x_{A \cup \{i\}} \in \gen{\pbcsp}$, and so we can replace $x_A$ with monomials of one higher degree. Repeatedly applying this up to degree $c$ (at which point we must stop to avoid dividing by zero), we determine there is a polynomial $p'$ which is homogenous of degree $c$ such that $p - p' \in \gen{\pbcsp}$. Now if $p'_{i_1,\dots,i_c}x_{i_1}\dots x_{i_c}$ is a monomial of $p'$, let $\alpha$ be the element of $V(\pbcsp)$ with $i_1,\dots,i_c$ coordinates equal to $1$ and all other coordinates equal to zero. Then $p'(\alpha) = p'_{i_1,\dots,i_c}$, but $p'(\alpha) = 0$. Thus in fact $p' = 0$, and so $p \in \gen{\pbcsp}$.
\end{proof}

\subsection{Symmetric Polynomials}
The first step is to show that any symmetrized polynomial can be derived from a constant polynomial in low degree. It is considerably simpler in this case, as the fundamental theorem of symmetric polynomials tells us that powers of $\sum_i x_i$ generate all the symmetric polynomials. 
\begin{lemma}\label{lem:bcsp-symmetric}
Let $p$ be a multilinear polynomial in $\R^n$. Then there exists a constant $c_p$ such that $p' = \sum_{\sigma \in \cS_n} \sigma p \cong c_p$, and the derivation can be done in degree $\deg p$. 
\end{lemma}
\begin{proof}
It is sufficient to prove the lemma for monomials $x_A = \prod_{i \in A} x_i$. We will induct on the degree of the monomial $|A|$. 
If $|A| = 1$, then $p = x_i$ for some $i \in [n]$, and $p' = \sum_{\sigma \in \cS_n} \sigma x_i = (n-1)!\sum_i x_i \cong (n-1)!\cdot c$, which can clearly be performed in degree one. Now assume $|A| = k$, so that $p' = \sum_{\sigma \in \cS_n} \sigma x_A = (n-k)! \sum_{|B|=k} x_B$. Then $p'' = p' - \frac{(n-k)!}{k!}\left(\sum_i x_i - c\right)^k$ is a polynomial which, after multilinearizing by reducing by the boolean constraints, has degree at most $k-1$. Furthermore, $p''$ is clearly in $\gen{\pbcsp}$ and is fixed by every $\sigma$. Thus by the inductive hypothesis, $p''$ has a derivation from some constant in degree $k-1$. Since $p' \cong p''$ in degree $k$, this implies the statement for $|A| = k$ and completes the proof by induction.
\end{proof}
By the same argument as \prettyref{cor:constantiszero}, we obtain the same result for \textsc{Balanced-CSP} constraints. We quickly note that \prettyref{lem:bcsp-symmetric} works regardless of the value of $c$, so we will reuse it when we consider the case of general $c$. Now we move on to the second step.

\subsection{Getting to a Symmetric Polynomial}
Recall the second step of our strategy is to show that $p - \sigma p$ can be derived from $\pbcsp$ in low degree. It will be easier in this case as compared to \textsc{Matching} because we do not have to increase the degree of $p - \sigma p$ in order to isolate a variable to remove and do the induction. Because of this, we will be able to show that \textsc{Bisection} is actually $1$-effective and we will not lose a factor of two this time.
We need a lemma to help us do the induction:
\begin{lemma}\label{lem:bcsp-induct}
Let $L$ be a polynomial with a degree $d$ derivation from $\pbcsp(n,c)$. Then $L\cdot (x_{n+1} - x_{n+2})$ has a degree $d+1$ derivation from $\pbcsp(n+2,c+1)$. 
\end{lemma}
\begin{proof}
It is sufficient to prove the lemma for $L \in \pbcsp(n,c)$. If $L = x_i^2 - x_i$, then $L \in \pbcsp(n+2,c+1)$ and so the lemma is clearly true. If $L = \sum_{i=0}^n x_i - c$, then 
\begin{align*}
L(x_{n+1} - x_{n+2}) &= \left(\sum_{i=0}^n x_i - c\right)(x_{n+1} - x_{n+2}) \\
&= \left(\sum_{i=0}^{n+2} x_i - (c+1)\right)(x_{n+1} - x_{n+2}) - (x_{n+1}^2 - x_{n+1}) - (x_{n+2}^2 - x_{n+2})
\end{align*}
and note that each of the three terms contains a factor from $\pbcsp(n+2,c+1)$ and the degree is never more than $2$. 
\end{proof}
We are now ready to prove that the \textsc{Bisection} constraints admit effective derivations.
\begin{theorem}\label{thm:bisec-effective}
Let $n$ be even and $p \in \gen{\pbcsp(n,n/2)}$ and $d = \deg p$. Then $p$ has a derivation from $\pbcsp(n,n/2)$ in degree $d$. 
\end{theorem}
\begin{proof}
By reducing by the boolean constraints, we can assume $p$ is a multilinear polynomial. 
We will induct on the number of vertices $n$, so first we must handle the base case of $n = 2$ (recall $n$ is even). 
The only degree zero polynomial in $\pbcsp(2)$ is the zero polynomial which has the trivial derivation. 
If $p = ax_1 + bx_2 + c$, we know $p(0,1) = p(1,0) = 0$. This implies $p$ is a multiple of $\sum_i x_i - 1$, which clearly has a derivation of degree $1$.
Finally, $x_1x_2$ has the derivation $x_1x_2 = x_1(x_1+x_2-1) + (-1)\cdot(x_1^2 - x_1)$.

Now assume the theorem statement for $\pbcsp(n')$ with $n' < n$. 
Let $\sigma = (i,j)$ be a transposition between two vertices. 
We consider the polynomial $\Delta = p - \sigma p$. 
We can decompose $p = r_i x_i + r_j x_j + r_{ij} x_ix_j + q_{ij}$, where each of the polynomials $r_i$, $r_j$, $r_{ij}$, and $q_{ij}$ depend on neither $x_i$ nor $x_j$. Then $\Delta = (r_i - r_j)(x_i - x_j)$. Now since $\Delta \in \gen{\pbcsp(n)}$, we know that $\Delta(x) = 0$ for any $x \in \{0,1\}^n$ with exactly $n/2$ indices which are $1$. In particular, if we set $x_i = 1$ and $x_j = 0$, we know that $(r_i - r_j)$ must be zero if the remaining variables are set so that they have exactly $n/2 - 1$ indices which are $1$. In other words, $(r_i - r_j)$ is zero on $V(\pbcsp(n-2))$. By \prettyref{lem:bcsp-complete}, we have $(r_i - r_j) \in \gen{\pbcsp(n-2)}$, and thus by the inductive hypothesis $(r_i - r_j)$ has a derivation from $\pbcsp(n-2)$ in degree $d-1$. By \prettyref{lem:bcsp-induct}, $\Delta = (r_i - r_j)(x_i - x_j)$ has a derivation from $\pbcsp(n)$ in degree $d$. 

Now since the transpositions generate the entire symmetric group, we have $p \cong \frac{1}{n!} \sum_{\sigma \in \cS_n} \sigma p \cong 0$, where the last congruence is by \prettyref{lem:bcsp-symmetric}. Thus $p$ has a derivation from $\pbcsp$ in degree $d$. 
\end{proof}

\subsection{Obstacles for General $c$}
Now that we are changing $c$ as well as $n$, we will write $\pbcsp(n,c)$ for the \textsc{Balanced-CSP} constraints on $n$ vertices with balance $c$.
We want to prove that these constraints admit effective derivations.
What goes wrong if we just try to imitate the proof of \prettyref{thm:bisec-effective}?
If we do so, eventually we arrive at the base case of the induction: $\pbcsp(n-2c,0)$.
The problem is that the linear monomials $x_i$ are in $\gen{\pbcsp(n-2c,0)}$ but it is not obvious how to derive $x_i$ from $\pbcsp(n-2c,0)$. 
In fact, it turns out that derivations of $x_i$ require degree $(n-2c+1)/2$. 

This is not an artifact of our proof strategy, there are essentially two kinds of polynomials in $\gen{\pbcsp(n,c)}$:
Polynomials of degree at most $c$, and polynomials of degree $c+1$ or greater. 
The former have efficient derivations:
\begin{lemma}\label{lem:bcsp-lowdeg}
Let $p \in \gen{\pbcsp(n,c)}$ have degree at most $c$. Then $p$ has a derivation from $\pbcsp(n,c)$ in degree $\deg p$.
\end{lemma}
We delay the proof of this lemma until the next section. 
However, the polynomials of degree $c+1$ or greater actually have no derivations until degree $(n-c+1)/2$, so if $c << n$, then $\pbcsp(n,c)$ is not $k$-effective for any constant $k$.
We will see that this phenomenon is because of the fact that the Pigeonhole Principle requires high degree to prove in the polynomial calculus. 
The negation of the Pigeonhole Principle is the following set of constraints:
\begin{align*}
\nphp(m,n) &= \left\{x_{ij}^2 - x_{ij} | i \in [m], j \in [n]\right\} \\
&\cup \left\{\sum_j x_{ij} - 1 | i \in [m]\right\} \\
&\cup \left\{x_{ij}x_{ik} | i \in [m], j,k \in [n], j \neq k\right\} \\
&\cup \left\{x_{ij}x_{kj} | i,k \in [m], j \in [n], i \neq k\right\}
\end{align*}
$\nphp(m,n)$ asserts the existence of an injective mapping from $[m]$ into $[n]$. If $m > n$, then clearly there is no such mapping, so the set of constraints is unsatisfiable. This implies that $1 \in \gen{\nphp(m,n)}$. However, Razborov proved that any derivation of $1$ from $\nphp(m,n)$ has degree at least $n/2 + 1$ \cite{}.
This allows us to prove the following:
\begin{lemma}\label{lem:bcsp-highdeg-hard}
Let $p = x_1x_2\dots x_c x_{c+1}$. Then $p \in \gen{\pbcsp(n,c)}$, but any derivation of $p$ from $\pbcsp(n,c)$ has degree at least $(n-c+1)/2$. 
\end{lemma}
\begin{proof}
To see that $p \in \gen{\pbcsp(n,c)}$, notice that $V(\pbcsp(n,c))$ are the boolean vectors with exactly $c$ variables equal to $1$.
Because $p$ is a product of $c+1$ variables, at least one of those variables must be equal to $0$, and thus the product is $0$. 
This is essentially a Pigeonhole Principle argument where the pigeons are the $n-c$ zeros, and the holes are the $n-c-1$ variables not appearing in $p$.
More formally, we show how to manipulate any derivation of $p$ from $\pbcsp(n,c)$ to get a derivation of $1$ from $\nphp(n-c,n-c-1)$.

Any derivation of $p$ from $\pbcsp(n,c)$ is a polynomial identity of the following form:
\[x_1x_2\dots x_{c+1} = \lambda \cdot (\sum_i x_i - c) + \sum_i \lambda_i \cdot (x_i^2 - x_i).\]
Now set $x_1 = x_2 = \dots = x_{c+1} = 1$ to get
\[1 = \lambda' \cdot (\sum_{i > c+1} x_i + 1) + \sum_{i > c+1} \lambda_i \cdot (x_i^2 - x_i).\]
We define variables $y_{ij}$ with the intention that $y_{ij} = 1$ if the $i$th variable is the $j$th zero. Thus we replace $x_i \rightarrow 1 - \sum_{j=1}^{n-c} y_{ij}$ and get
\begin{align*}
1 &= \lambda(y) \cdot \left(\sum_{i > c+1} \left(1 - \sum_j y_{ij}\right) + 1\right) + \sum_{i > c+1} \lambda_i(y) \cdot \left( \left(1 - \sum_j y_{ij}\right)^2 - 1 + \sum_j y_{ij}\right) \\
&= \lambda(y) \cdot \left(n-c-1 - \sum_{i> c+1, j} y_{ij} + 1\right) + \sum_{i > c+1} \left(\sum_j y_{ij}^2 - \sum_j y_{ij} + 2\sum_{j\neq j'} y_{ij}y_{ij'}\right) \\
&= \sum_{j=1}^{n-c} -\lambda(y)\cdot\left(\sum_{i > c+1} y_{ij} - 1\right) + \sum_{i > c+1} \left(\sum_j \left(y_{ij}^2 - y_{ij}\right) + 2\sum_{j\neq j'} y_{ij}y_{ij'}\right).
\end{align*}
Note that each term in the last equation contains a constraint in $\nphp(n-c,n-c-1)$. Thus the degree of this derivation must be at least  $(n-c+1)/2$.
\end{proof}

\subsection{Effective PC$_>$ Derivations for \textsc{Balanced-CSP}}
\prettyref{lem:bcsp-highdeg-hard} tells us that we cannot hope to prove that $\pbcsp(n,c)$ has effective PC proofs, but we are not soley interested in PC proofs. In particular, because the applications we consider in this thesis are primarily focused on Semidefinite Programming, we have access to the more powerful PC$_>$ proof system. In this system, $\nphp$ is not difficult to refute, and indeed once we allow ourselves PC$_>$ proofs we can show that \textsc{Balanced-CSP} admits effective derivations.
\begin{lemma}\label{lem:highdeg-easy}
Let $p = x_1x_2\dots x_c x_{c+1}$. Then $p$ has a PC$_>$ proof from $\pbcsp(n,c)$ in degree $2(c+1)$ with coefficient size at most $O(1)$. 
\end{lemma}
\begin{proof}
Recall that a PC$_>$ proof consists of two proofs of nonnegativity: one for $p$ and one for $-p$. The first is trivial: every monomial is the multilinearization of itself squared. Thus every monomial has a proof of nonnegativity in twice its degree. For the second, we observe the following identity
\[-x_1x_2\dots x_{c+1} = x_1x_2\dots x_c\left(c - \sum_i x_i\right) + \sum_{i \leq c} -(x_i^2 - x_i) \prod_{j \leq c, j\neq i} x_j + \left(\prod_{i \leq c} x_i\right) \sum_{i > c+1} x_i. \]
The first two terms each have factors in $\pbcsp(n,c)$, and the last term is a sum of monomials with nonnegative coefficients. These monomials all have proofs of nonnegativity, and thus so does $-p$. It is simple to check that these proofs involve coefficients of merely constant size. 
\end{proof}
It remains to prove that the low-degree polynomials in $\pbcsp(n,c)$ have efficient derivations. We will be able to use simple PC derivations for these polynomials. The proof is very similar to the one for \textsc{Bisection}, but we have to do a double induction on $n$ and $c$ since the balance changes in the inductive step. We will take $c \leq n/2$, since the other case is symmetric.
\begin{lemma}\label{lem:lowdeg-easy}
Fix $c \leq n/2$. Let $p \in \gen{\pbcsp(n,c)}$ with $\deg p \leq c$. Then $p$ has a derivation from $\pbcsp(n,c)$ in degree $\deg p$. 
\end{lemma}
\begin{proof}
The proof is by double induction on $n$ and $c$. The base case is the lemma statement for $\pbcsp(n,0)$ for all $n$. In this case $p$ is a constant polynomial, and the only constant polynomial in $\pbcsp(n,0)$ is the zero polynomial, which has the trivial derivation. Now consider the case when $p \in \pbcsp(n,c)$ for $c \leq n/2$. Then following the same argument as in \prettyref{thm:bisec-effective}, we define $\Delta = p - \sigma p = (r_i - r_j)\cdot (x_i - x_j)$, where $r_i$ and $r_j$ do not depend on $x_i$ or $x_j$. Setting $x_i = 1$ and $x_0 = 0$, we again conclude that $(r_i - r_j) \in \gen{\pbcsp(n-2,c-1)}$. Since $c \leq n/2$, clearly $c-1 \leq (n-2)/2$. Also, since $r_i - r_j$ has degree $\deg p - 1$, we still have $\deg (r_i - r_j) \leq c-1$. Thus we can apply the inductive hypothesis to get a derivation for $r_i - r_j$ from $\pbcsp(n-2,c-1)$ in degree $\deg p - 1$. Then \prettyref{lem:bcsp-induct} tells us that $\Delta$ has a derivation from $\pbcsp(n,c)$ in degree $\deg p$, completing the induction. Taken with \prettyref{lem:bcsp-symmetric}, this implies the statement. 
\end{proof}
Together, we now have a proof of the following theorem:
\begin{theorem}
\textsc{Balanced-CSP} on $n$ vertices with balance $c$ admits $(2,O(1))$-effective PC$_>$ derivations. If $c = n/2$, then the derivations can be taken to be $1$-effective PC derivations.
\end{theorem}
\begin{proof}
The theorem is a direct corollary of \prettyref{lem:highdeg-easy}, \prettyref{lem:lowdeg-easy}, and \prettyref{lem:bisec-effective}.
\end{proof}

\section{\textsc{Boolean Sparse PCA}}
The last example we give for our proof strategy is the \textsc{Boolean Sparse PCA} problem, whose constraints are given by 
\begin{align}
\pspca(n,c) = \left\{x_i^3 - x_i | i \in [n]\right\} \cup \left\{\sum_i x_i^2 - c\right\}.\label{eq:bspca-formulation}
\end{align}
This problem arises when trying to reconstruct a planted sparse vector from noisy samples, see for example \cite{Ma, Wigderson}.

These constraints are similar to the \textsc{Balanced-CSP} constraints of the previous section with one crucial difference: The variables are ternary instead of binary.
This will complicate the analysis, but it turns out that with a bit more casework we will be able to push it through.
However, the Pigeonhole Principle obstacle remains, and we will once again only be able to prove that low-degree polynomials have effective PC derivations.
\begin{lemma}\label{lem:bspca-complete}
$\pspca(n,c)$ is complete for every $n$ and $c \leq n$.
\end{lemma}
\begin{proof}
Recall that by pigeonhole principle any monomial that involves $c+1$ or more distinct variables will be zero over $V(\pspca)$. Our first step is to show that these are in $\gen{\pspca}$. The proof will be a reverse induction on the number of distinct variables, going from $n$ to $c+1$.  
For the base case, let $x_A$ be any monomial with $n$ distinct variables. Then $x_A \cong \frac{1}{n-c} x_A(\sum_i x_i^2 - c)$, so clearly $x_A \in \gen{\pspca}$.
Now let $x_A$ be any monomial with $c+1 \leq k < n$ distinct variables. Then
\[\left(k-c\right)x_A + \sum_{i \notin A} x_{A \cup \{i,i\}} \cong x_A(\sum_i x^2_i - c).\]
By the inductive hypothesis, the second term is in $\gen{\pspca}$, and thus so is $x_A$. 
Now let $p$ be a polynomial such that $p(\alpha) = 0$ for every $\alpha \in V(\pspca)$. 
We can assume that the monomials of $p$ involve at most $c$ distinct variables. For any monomial $x_A$ of $p$, we have $x_A(\sum_i x_i - 1) \cong \sum_{i \notin A} x_{A \cup \{i,i\}} - (c-|A|)x_A$, and so $x_A - \frac{1}{c - |A|}\sum_{i \notin A} x_{A \cup \{i,i\}} \in \gen{\pspca}$, and so we can replace $x_A$ with monomials of one higher degree. Repeatedly applying this up to degree $c$ (at which point we must stop to avoid dividing by zero), we determine there is a polynomial $p'$ which has only monomials involving exactly $c$ distinct variables such that $p - p' \in \gen{\pspca}$. Fix two disjoint sets $U_1$ and $U_2$ of the variables with $|U_1 \cup U_2| = c$ and let $p'_{U_1U_2}$ be the coefficient of the monomial of $p'$ corresponding to the variables in $U_1 \cup U_2$ with the variables in $U_1$ appearing with degree one and the variables in $U_2$ appearing with degree two. We will prove by induction that $p'_{U_1U_2} = 0$ for every $U_1,U_2$. For the base case, let $U_1 = \emptyset$. Then if we average every monomial of $p'$ over the $\alpha \in V(\pspca)$ that assign nonzero values exactly to the variables in $U_1 \cup U_2$, every monomial except $p'_{U_1U_2}$ is zero, and that monomial has value one. Since $p'(\alpha) = 0$ for each $\alpha \in V(\pspca)$, this implies that $p'_{U_1U_2} = 0$. Proceeding by induction, let $|U_1| = k$. Then if we average over all the $\alpha \in V(\pspca)$ that assign nonzero values exactly to the variables in $U_1 \cup U_2$ and assigns value $1$ to the variables in $U_1$, every monomial is zero except $p'_{UV}$ with $U \cup V = U_1 \cup U_2$ and $U \subseteq U_1$. By the inductive hypothesis these all have zero coefficients except $p'_{U_1U_2}$, and now since $p'$ is zero on all these points, we once again have $p'_{U_1U_2}$. Doing this for every $U_1,U_2$, we determine $p' = 0$ and thus $p \in \gen{\pspca}$. 
\end{proof}

\subsection{Symmetric Polynomials}
Once again, we prove a derivation lemma for symmetric polynomials. For this set of constraints, it is not as simple as saying that every symmetric polynomial is equal to some constant on $V(\pspca)$ because we only have a constraint on $\sum_i x_i^2$ as opposed to $\sum_i x_i$. In particular, the polynomial $\sum_i x_i$ itself does not reduce to a constant on $V(\pspca)$. We will have to make a slightly more general argument.
\begin{lemma}\label{lem:bpca-symmetric}
Let $p$ be a polynomial in $\R^n$. Then there exists a univariate polynomial $q$ of degree $\deg p$ such that $p' = \frac{1}{n!}\sum_{\sigma \in \cS_n} \sigma p \cong q(\sum_i x_i)$.
\end{lemma}
\begin{proof}
We prove that for every elementary symmetric polynomial $e_k(x)$, there exists a univariate polynomial $q_k$ such that $e_k(x) - q_k(\sum_i x_i)$ has a derivation from $\pspca$ in degree $k$, then the fundamental theorem of symmetric polynomials implies the lemma. For the base case, clearly $q_0(t) = 1$ and $q_1(t) = t$. For the general case, consider the terms of the expansion of $\left(\sum_i x_i\right)^k$. They are indexed by the nonincreasing partitions of $k$: $\lambda = (\lambda_1, \lambda_2, \dots, \lambda_\ell)$ and can be written $c_\lambda \sum_{i_1,\dots,i_\ell} x_{i_1}^{\lambda_1} x_{i_2}^{\lambda_2} \dots x_{i_\ell}^{\lambda_\ell}$. Now just by reducing by $x_i^3 - x_i$, we can reduce the exponents on each variable to either one or two. If any exponent is two, then by reducing by the constraint $\sum_i x_i^2 - c$, we can replace any of these exponents with a multiplicative constant. Thus after reducing, all of the exponents are one. But now this term is simply a multiple of some $e_{k'}(x)$, with $k' \leq k$. Since one term is exactly $k!e_k(x)$, we have
\[\frac{1}{k!}\left(\sum_i x_i\right)^k - e_k(x) \cong \sum_{i=1}^{k-1} a_i e_i(x)\]
for some real numbers $a_i$. Now by the inductive hypothesis, we know that there exist polynomials $q_i$ such that $e_i(x) - q_i(\sum_i x_i)$ has a derivation from $\pspca$ in degree $i$. Thus we set $\frac{1}{k!}q_k(t) = t^k - \sum_i a_i q_i(t)$ to complete the induction and the lemma.
\end{proof}
\begin{corollary}
Let $p \in \gen{\pspca}$ with $\deg p \leq c$. Then $p' = \frac{1}{n!}\sum_{\sigma \in \cS_n} \sigma p$ has a derivation from $\pspca$ in degree $\deg p$. 
\end{corollary}
\begin{proof}
By \prettyref{lem:bpca-symmetric}, we know that there is a univariate polynomial $q(t)$ of degree $\deg p$ such that $p' \cong q(\sum_i x_i)$. Since $p \in \gen{\pspca}$, so is $p'$ and $q(\sum_i x_i)$. Since there are $c+1$ possible values of $\sum_i x_i$ in $V(\pspca)$, namely $\{-c, -c+2, \dots, c-2, c\}$, $q$ has $c+1$ zeros. But $\deg q = \deg p \leq c$, so $q$ must be the zero polynomial.
\end{proof}

\subsection{Getting to a Symmetric Polynomial}
This process should be familiar by now. Since there are more choices for values for the variables we are going to strip off, we are going to need to do a little more casework, but the general strategy is the same. We start with a lemma that allows us to perform induction.
\begin{lemma}\label{lem:bcpa-induct}
Let $L$ be a polynomial with a degree $d$ derivation from $\pspca(n,c)$. Then $L(x_{n+1}^2-x_{n+2}^2)$ has a degree $d+2$ derivation from $\pspca(n+2,c+1)$, and $L(x_{n+1}x_{n+2})$ has a degree $d+2$ derivation from $\pspca(n+2,c+2)$.
\end{lemma}
\begin{proof}
It suffices to prove the theorem for $L \in \pspca(n,c)$. If $L = x_i^3 - x_i$ for some $i$, then clearly the statement is true as $L \in \pspca(n+2,c+1)$ and $L \in \pspca(n+2,c+2)$, so let $L = \sum_i x_i^2 - c$. Now notice that 
\begin{align*}
L(x_{n+1}^2 - x_{n+2}^2) - \left(\sum_{i=1}^{n+2} x_i^2 - (c+1)\right)(x_{n+1}^2-x_{n+2}^2) &= (1-x_{n+1}^2+x_{n+2}^2)(x_{n+1}^2-x_{n+2}^2) \\
&= x_{n+1}^2 - x_{n+2}^2 - x_{n+1}^4 + x_{n+2}^4  \\
&\cong x_{n+1}^2 - x_{n+2}^2 - x_{n+1}^2 + x_{n+2}^2 \\
&= 0
\end{align*}
and 
\begin{align*}
Lx_{n+1}x_{n+2} - \left(\sum_{i=1}^{n-2} x_i^2 - (c+2)\right)x_{n+1}x_{n+2} &= (2-x_{n+1}^2 + x_{n+2}^2)x_{n+1}x_{n+2} \\
&= 2x_{n+1}x_{n+2} - x_{n+1}^3x_{n+2} + x_{n+1}x_{n+2}^3 \\
&\cong 2x_{n+1}x_{n+2} - x_{n+1}x_{n+2} + x_{n+1}x_{n+2} \\
&= 0
\end{align*}
to conclude the lemma. 
\end{proof}
Now we prove that \textsc{Boolean Sparse PCA} admits effective derivations for low degree polynomials.
\begin{lemma}
Fix $c \leq n/2$. Let $p \in \gen{\pspca(n,c)}$ with $\deg p \leq c/2$. Then $p$ has a derivation from $\pspca(n,c)$ in degree at most $3\deg p$.
\end{lemma}
\begin{proof}
We do double induction on $n$ and $c$. For the base case of $\pspca(n,0)$, note that the only polynomial with degree at most $0$ is the constant polynomial $0$, which has the trivial derivation. Now let $p$ have degree at most $d \leq c$. We can assume the individual degree of each variable is at most two by reducing by the ternary constraints. Following the same argument as in \prettyref{thm:bisec-effective}, we define the polynomial $\Delta = p - \sigma p$ for the transposition $\sigma = (i,j)$, but now since $p$ is not multilinear, we write it as 
\[p = r_{10}x_i + r_{01}x_j + r_{11}x_ix_j + r_{21}x_i^2x_j + r_{12}x_ix_j^2 + r_{22}x_i^2x_j^2 + q_{ij}\]
where none of the $r$ or $q$ polynomials depend on $x_i$ or $x_j$. Then $\Delta$ can be written
\begin{align*}
\Delta &= (r_{10} - r_{01})(x_i - x_j) + (r_{20} - r_{02})(x_i^2 - x_j^2) + (r_{21} - r_{12})(x_i^2x_j - x_ix_j^2) \\
&= ((r_{10} - r_{01}) + (r_{20} - r_{02})(x_i+x_j) + (r_{21} - r_{12})x_ix_j)(x_i - x_j) \\
&= (R_0 + R_1(x_i + x_j) + R_2x_ix_j)(x_i - x_j)
\end{align*}
where we define $R_0 = (r_{10} - r_{01})$, $R_1 = (r_{20} - r_{02})$, and $R_2 = (r_{21} - r_{12})$, and note that they are polynomials of degree at most $d-1$. If we set $x_i = 1$ and $x_j = 0$, we obtain a polynomial $R_0 + R_1$ which must be zero on $V(\pspca(n-2,c-1))$. Furthermore, if we set $x_i = -1$ and $x_j = 0$, then $R_0 - R_1$ is zero on $V(\pspca(n-2,c-1))$, and setting $x_i = 1$ and $x_j = -1$, we also get that $R_0 - R_2$ is zero on $V(\pspca(n-2,c-2))$. 

Since $c \leq n/2$, clearly $c-2 \leq c-1 \leq (n-2)/2$. Since $d \leq c/2$, we also have $d-1 \leq (c-2)/2$. Since by \prettyref{lem:bspca-complete} we know $\pspca(n,c)$ is complete, we can apply the inductive hypothesis and so all these polynomials have derivations of degree at most $3(d-1)$ from their constraints. By \prettyref{lem:bpca-induct}, we know $(R_0+R_1)(x_i^2-x_j^2)$, $(R_0 - R_1)(x_i^2-x_j^2)$, and $(R_0 - R_2)x_ix_j$ have derivations from $V(\pspca(n,c))$ in degree $3d-1$.

From the first two polynomials, it is clear that $R_0(x_i^2-x_j^2)$ and $R_1(x_i^2-x_j^2)$ have derivations in degree $3d-1$. We also have
\begin{align*}
R_0(x_i^2-x_j^2) \cdot (x_i+x_j) &- (R_0 - R_2)x_ix_j \cdot (x_i - x_j) = \\&= R_0((x_i^2 - x_j^2)(x_i+x_j) - x_ix_j(x_i - x_j)) + R_2x_ix_j(x_i - x_j) \\
&\cong (R_0 + R_2x_ix_j)(x_i - x_j)
\end{align*}
and thus $(R_0 + R_2x_ix_j)(x_i - x_j)$ is derivable in degree $3d$. This implies that $\Delta$ has a derivation in degree $3d$, and taken together with \prettyref{lem:bpca-symmetric}, we conclude that $p$ has a derivation in degree $3d$. 
\end{proof}

\section{Optimization Problems with Effective Derivations}
We include a corollary here summarizing all the results of this section:
\begin{corollary}\label{cor:effective_list}
The following polynomial formulations of combinatorial optimization problems admit $k$-effective derivations:
\begin{itemize}
\item \textsc{CSP}: $\pcsp(n) = \csp$, $k = 1$.
\item \textsc{Clique}: $\pclique(V,E) = \clique$, $k = 1$.
\item \textsc{Matching}: $\pmatch(n) = \matching$, $k = 2$.
\item \textsc{TSP}: $\ptsp(n) = \tsp$, $k = 2$.
\item \textsc{Bisection}: $\pbcsp(n,n/2) = \bisec$, $k = 1$.
\end{itemize}
The following sets of constraints admit $k$-effective derivations up to degree $c$:
\begin{itemize}
\item \textsc{Balanced CSP}: $\pbcsp(n,c) = \bcsp{n}{c}$, $k = 1$.
\item \textsc{Boolean Sparse PCA}: $\pspca(n,c) = \bspca{n}{2c}$, $k = 3$.
\end{itemize}
\end{corollary}
